
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Backpropagation &#8212; üè† ML Glossary</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/theme_overrides.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Activation Functions" href="activation_functions.html" />
    <link rel="prev" title="Forwardpropagation" href="forwardpropagation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"><br/>
<a href="https://github.com/bfortuner/ml-glossary">
    
    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 496 512"><!-- Font Awesome Pro 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    Edit on GitHub
</a></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">üè† ML Glossary</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="linear_regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient_descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic_regression.html">
   Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   Glossary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="calculus.html">
   Calculus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability.html">
   Probability (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statistics.html">
   Statistics (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="math_notation.html">
   Notation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nn_concepts.html">
   Concepts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="forwardpropagation.html">
   Forwardpropagation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="activation_functions.html">
   Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loss_functions.html">
   Loss Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optimizers.html">
   Optimizers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regularization.html">
   Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="architectures.html">
   Architectures
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Algorithms (TODO)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="classification_algos.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="clustering_algos.html">
   Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_algos.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="reinforcement_learning.html">
   Reinforcement Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="datasets.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="libraries.html">
   Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="papers.html">
   Papers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="other_content.html">
   Other
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="contribute.html">
   How to contribute
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/backpropagation.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chain-rule-refresher">
   Chain rule refresher
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applying-the-chain-rule">
   Applying the chain rule
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-work-with-memoization">
   Saving work with memoization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-example">
   Code example
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Backpropagation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chain-rule-refresher">
   Chain rule refresher
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applying-the-chain-rule">
   Applying the chain rule
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#saving-work-with-memoization">
   Saving work with memoization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#code-example">
   Code example
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="backpropagation">
<span id="id1"></span><h1>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this headline">#</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#chain-rule-refresher" id="id3">Chain rule refresher</a></p></li>
<li><p><a class="reference internal" href="#applying-the-chain-rule" id="id4">Applying the chain rule</a></p></li>
<li><p><a class="reference internal" href="#saving-work-with-memoization" id="id5">Saving work with memoization</a></p></li>
<li><p><a class="reference internal" href="#code-example" id="id6">Code example</a></p></li>
</ul>
</div>
<p>The goals of backpropagation are straightforward: adjust each weight in the network in proportion to how much it contributes to overall error. If we iteratively reduce each weight‚Äôs error, eventually we‚Äôll have a series of weights that produce good predictions.</p>
<section id="chain-rule-refresher">
<h2><a class="toc-backref" href="#id3">Chain rule refresher</a><a class="headerlink" href="#chain-rule-refresher" title="Permalink to this headline">#</a></h2>
<p>As seen above, foward propagation can be viewed as a long series of nested equations. If you think of feed forward this way, then backpropagation is merely an application of <a class="reference internal" href="calculus.html#chain-rule"><span class="std std-ref">Chain rule</span></a> to find the <a class="reference internal" href="calculus.html#derivative"><span class="std std-ref">Derivatives</span></a> of cost with respect to any variable in the nested equation. Given a forward propagation function:</p>
<div class="math notranslate nohighlight">
\[f(x) = A(B(C(x)))\]</div>
<p>A, B, and C are activation functions at different layers. Using the chain rule we easily calculate the derivative of <span class="math notranslate nohighlight">\(f(x)\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[f'(x) = f'(A) \cdot A'(B) \cdot B'(C) \cdot C'(x)\]</div>
<p>How about the derivative with respect to B? To find the derivative with respect to B you can pretend <span class="math notranslate nohighlight">\(B(C(x))\)</span> is a constant, replace it with a placeholder variable B, and proceed to find the derivative normally with respect to B.</p>
<div class="math notranslate nohighlight">
\[f'(B) = f'(A) \cdot A'(B)\]</div>
<p>This simple technique extends to any variable within a function and allows us to precisely pinpoint the exact impact each variable has on the total output.</p>
</section>
<section id="applying-the-chain-rule">
<h2><a class="toc-backref" href="#id4">Applying the chain rule</a><a class="headerlink" href="#applying-the-chain-rule" title="Permalink to this headline">#</a></h2>
<p>Let‚Äôs use the chain rule to calculate the derivative of cost with respect to any weight in the network. The chain rule will help us identify how much each weight contributes to our overall error and the direction to update each weight to reduce our error. Here are the equations we need to make a prediction and calculate total error, or cost:</p>
<img alt="_images/backprop_ff_equations.png" class="align-center" src="_images/backprop_ff_equations.png" />
<p>Given a network consisting of a single neuron, total cost could be calculated as:</p>
<div class="math notranslate nohighlight">
\[Cost = C(R(Z(X W)))\]</div>
<p>Using the chain rule we can easily find the derivative of Cost with respect to weight W.</p>
<div class="math notranslate nohighlight">
\[\begin{split}C'(W) &amp;= C'(R) \cdot R'(Z) \cdot Z'(W) \\
      &amp;= (\hat{y} -y) \cdot R'(Z) \cdot X\end{split}\]</div>
<p>Now that we have an equation to calculate the derivative of cost with respect to any weight, let‚Äôs go back to our toy neural network example above</p>
<img alt="_images/simple_nn_diagram_zo_zh_defined.png" class="align-center" src="_images/simple_nn_diagram_zo_zh_defined.png" />
<p>What is the derivative of cost with respect to <span class="math notranslate nohighlight">\(W_o\)</span>?</p>
<div class="math notranslate nohighlight">
\[\begin{split}C'(W_O) &amp;= C'(\hat{y}) \cdot \hat{y}'(Z_O) \cdot Z_O'(W_O) \\
        &amp;= (\hat{y} - y) \cdot R'(Z_O) \cdot H\end{split}\]</div>
<p>And how about with respect to <span class="math notranslate nohighlight">\(W_h\)</span>? To find out we just keep going further back in our function applying the chain rule recursively until we get to the function that has the Wh term.</p>
<div class="math notranslate nohighlight">
\[\begin{split}C'(W_h) &amp;= C'(\hat{y}) \cdot O'(Z_o) \cdot Z_o'(H) \cdot H'(Z_h) \cdot Z_h'(W_h) \\
        &amp;= (\hat{y} - y) \cdot R'(Z_o) \cdot W_o \cdot R'(Z_h) \cdot X\end{split}\]</div>
<p>And just for fun, what if our network had 10 hidden layers. What is the derivative of cost for the first weight <span class="math notranslate nohighlight">\(w_1\)</span>?</p>
<div class="math notranslate nohighlight">
\[\begin{split}C'(w_1) = \frac{dC}{d\hat{y}} \cdot \frac{d\hat{y}}{dZ_{11}} \cdot \frac{dZ_{11}}{dH_{10}} \cdot \\ \frac{dH_{10}}{dZ_{10}} \cdot \frac{dZ_{10}}{dH_9} \cdot \frac{dH_9}{dZ_9} \cdot \frac{dZ_9}{dH_8} \cdot \frac{dH_8}{dZ_8} \cdot \frac{dZ_8}{dH_7} \cdot \frac{dH_7}{dZ_7} \cdot \\ \frac{dZ_7}{dH_6} \cdot \frac{dH_6}{dZ_6} \cdot \frac{dZ_6}{dH_5} \cdot \frac{dH_5}{dZ_5} \cdot \frac{dZ_5}{dH_4} \cdot \frac{dH_4}{dZ_4} \cdot \frac{dZ_4}{dH_3} \cdot \\ \frac{dH_3}{dZ_3} \cdot \frac{dZ_3}{dH_2} \cdot \frac{dH_2}{dZ_2} \cdot \frac{dZ_2}{dH_1} \cdot \frac{dH_1}{dZ_1} \cdot \frac{dZ_1}{dW_1}\end{split}\]</div>
<p>See the pattern? The number of calculations required to compute cost derivatives increases as our network grows deeper. Notice also the redundancy in our derivative calculations. Each layer‚Äôs cost derivative appends two new terms to the terms that have already been calculated by the layers above it. What if there was a way to save our work somehow and avoid these duplicate calculations?</p>
</section>
<section id="saving-work-with-memoization">
<h2><a class="toc-backref" href="#id5">Saving work with memoization</a><a class="headerlink" href="#saving-work-with-memoization" title="Permalink to this headline">#</a></h2>
<p>Memoization is a computer science term which simply means: don‚Äôt recompute the same thing over and over. In memoization we store previously computed results to avoid recalculating the same function. It‚Äôs handy for speeding up recursive functions of which backpropagation is one. Notice the pattern in the derivative equations below.</p>
<img alt="_images/memoization.png" class="align-center" src="_images/memoization.png" />
<p>Each of these layers is recomputing the same derivatives! Instead of writing out long derivative equations for every weight, we can use memoization to save our work as we backprop error through the network. To do this, we define 3 equations (below), which together encapsulate all the calculations needed for backpropagation. The math is the same, but the equations provide a nice shorthand we can use to track which calculations we‚Äôve already performed and save our work as we move backwards through the network.</p>
<img alt="_images/backprop_3_equations.png" class="align-center" src="_images/backprop_3_equations.png" />
<p>We first calculate the output layer error and pass the result to the hidden layer before it. After calculating the hidden layer error, we pass its error value back to the previous hidden layer before it. And so on and so forth. As we move back through the network we apply the 3rd formula at every layer to calculate the derivative of cost with respect that layer‚Äôs weights. This resulting derivative tells us in which direction to adjust our weights to reduce overall cost.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The term <em>layer error</em> refers to the derivative of cost with respect to a layer‚Äôs <em>input</em>. It answers the question: how does the cost function output change when the input to that layer changes?</p>
</div>
<p class="rubric">Output layer¬†error</p>
<p>To calculate output layer error we need to find the derivative of cost with respect to the output layer input, <span class="math notranslate nohighlight">\(Z_o\)</span>. It answers the question‚Ää‚Äî‚Äähow are the final layer‚Äôs weights impacting overall error in the network? The derivative is then:</p>
<div class="math notranslate nohighlight">
\[C'(Z_o) = (\hat{y} - y) \cdot R'(Z_o)\]</div>
<p>To simplify notation, ml practitioners typically replace the <span class="math notranslate nohighlight">\((\hat{y}-y) * R'(Zo)\)</span> sequence with the term <span class="math notranslate nohighlight">\(E_o\)</span>. So our formula for output layer error equals:</p>
<div class="math notranslate nohighlight">
\[E_o = (\hat{y} - y) \cdot R'(Z_o)\]</div>
<p class="rubric">Hidden layer¬†error</p>
<p>To calculate hidden layer error we need to find the derivative of cost with respect to the hidden layer input, Zh.</p>
<div class="math notranslate nohighlight">
\[C'(Z_h) = (\hat{y} - y) \cdot R'(Z_o) \cdot W_o \cdot R'(Z_h)\]</div>
<p>Next we can swap in the <span class="math notranslate nohighlight">\(E_o\)</span> term above to avoid duplication and create a new simplified equation for Hidden layer error:</p>
<div class="math notranslate nohighlight">
\[E_h = E_o \cdot W_o \cdot R'(Z_h)\]</div>
<p>This formula is at the core of backpropagation. We calculate the current layer‚Äôs error, and pass the weighted error back to the previous layer, continuing the process until we arrive at our first hidden layer. Along the way we update the weights using the derivative of cost with respect to each weight.</p>
<p class="rubric">Derivative of cost with respect to¬†any weight</p>
<p>Let‚Äôs return to our formula for the derivative of cost with respect to the output layer weight <span class="math notranslate nohighlight">\(W_o\)</span>.</p>
<div class="math notranslate nohighlight">
\[C'(W_O) = (\hat{y} - y) \cdot R'(Z_O) \cdot H\]</div>
<p>We know we can replace the first part with our equation for output layer error <span class="math notranslate nohighlight">\(E_o\)</span>. H represents the hidden layer activation.</p>
<div class="math notranslate nohighlight">
\[C'(W_o) = E_o \cdot H\]</div>
<p>So to find the derivative of cost with respect to any weight in our network, we simply multiply the corresponding layer‚Äôs error times its input (the previous layer‚Äôs output).</p>
<div class="math notranslate nohighlight">
\[C'(w) = CurrentLayerError \cdot CurrentLayerInput\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>Input</em> refers to the activation from the previous¬†layer, not the weighted input, Z.</p>
</div>
<p class="rubric">Summary</p>
<p>Here are the final 3 equations that together form the foundation of backpropagation.</p>
<img alt="_images/backprop_final_3_deriv_equations.png" class="align-center" src="_images/backprop_final_3_deriv_equations.png" />
<p>Here is the process visualized using our toy neural network example above.</p>
<img alt="_images/backprop_visually.png" class="align-center" src="_images/backprop_visually.png" />
</section>
<section id="code-example">
<h2><a class="toc-backref" href="#id6">Code example</a><a class="headerlink" href="#code-example" title="Permalink to this headline">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">def</span> <span class="nf">relu_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="linenos"> 2</span>    <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos"> 3</span>        <span class="k">return</span> <span class="mi">1</span>
<span class="linenos"> 4</span>    <span class="k">return</span> <span class="mi">0</span>
<span class="linenos"> 5</span>
<span class="linenos"> 6</span><span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="linenos"> 7</span>    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">yHat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
<span class="linenos"> 8</span>
<span class="linenos"> 9</span><span class="k">def</span> <span class="nf">cost_prime</span><span class="p">(</span><span class="n">yHat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="linenos">10</span>    <span class="k">return</span> <span class="n">yHat</span> <span class="o">-</span> <span class="n">y</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="k">def</span> <span class="nf">backprop</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">Wo</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
<span class="linenos">13</span>    <span class="n">yHat</span> <span class="o">=</span> <span class="n">feed_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">Wo</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span>    <span class="c1"># Layer Error</span>
<span class="linenos">16</span>    <span class="n">Eo</span> <span class="o">=</span> <span class="p">(</span><span class="n">yHat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">relu_prime</span><span class="p">(</span><span class="n">Zo</span><span class="p">)</span>
<span class="linenos">17</span>    <span class="n">Eh</span> <span class="o">=</span> <span class="n">Eo</span> <span class="o">*</span> <span class="n">Wo</span> <span class="o">*</span> <span class="n">relu_prime</span><span class="p">(</span><span class="n">Zh</span><span class="p">)</span>
<span class="linenos">18</span>
<span class="linenos">19</span>    <span class="c1"># Cost derivative for weights</span>
<span class="linenos">20</span>    <span class="n">dWo</span> <span class="o">=</span> <span class="n">Eo</span> <span class="o">*</span> <span class="n">H</span>
<span class="linenos">21</span>    <span class="n">dWh</span> <span class="o">=</span> <span class="n">Eh</span> <span class="o">*</span> <span class="n">x</span>
<span class="linenos">22</span>
<span class="linenos">23</span>    <span class="c1"># Update weights</span>
<span class="linenos">24</span>    <span class="n">Wh</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dWh</span>
<span class="linenos">25</span>    <span class="n">Wo</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">dWo</span>
</pre></div>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets">1</span></dt>
<dd><p>Example</p>
</dd>
</dl>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="forwardpropagation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Forwardpropagation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="activation_functions.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Activation Functions</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Team<br/>
  
      &copy; Copyright 2017.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>