
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Logistic Regression &#8212; üè† ML Glossary</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/theme_overrides.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Glossary" href="glossary.html" />
    <link rel="prev" title="Gradient Descent" href="gradient_descent.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"><br/>
<a href="https://github.com/bfortuner/ml-glossary">
    
    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 496 512"><!-- Font Awesome Pro 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    Edit on GitHub
</a></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">üè† ML Glossary</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="linear_regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient_descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   Glossary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="calculus.html">
   Calculus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability.html">
   Probability (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statistics.html">
   Statistics (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="math_notation.html">
   Notation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nn_concepts.html">
   Concepts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="forwardpropagation.html">
   Forwardpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backpropagation.html">
   Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="activation_functions.html">
   Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loss_functions.html">
   Loss Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optimizers.html">
   Optimizers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regularization.html">
   Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="architectures.html">
   Architectures
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Algorithms (TODO)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="classification_algos.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="clustering_algos.html">
   Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_algos.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="reinforcement_learning.html">
   Reinforcement Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="datasets.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="libraries.html">
   Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="papers.html">
   Papers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="other_content.html">
   Other
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="contribute.html">
   How to contribute
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/logistic_regression.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparison-to-linear-regression">
     Comparison to linear regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-logistic-regression">
     Types of logistic regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binary-logistic-regression">
   Binary logistic regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sigmoid-activation">
     Sigmoid activation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-boundary">
     Decision boundary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#making-predictions">
     Making predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cost-function">
     Cost function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mapping-probabilities-to-classes">
     Mapping probabilities to classes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-evaluation">
     Model evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiclass-logistic-regression">
   Multiclass logistic regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#procedure">
     Procedure
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#softmax-activation">
     Softmax activation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scikit-learn-example">
     Scikit-Learn example
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Logistic Regression</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#comparison-to-linear-regression">
     Comparison to linear regression
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#types-of-logistic-regression">
     Types of logistic regression
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binary-logistic-regression">
   Binary logistic regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sigmoid-activation">
     Sigmoid activation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#decision-boundary">
     Decision boundary
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#making-predictions">
     Making predictions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cost-function">
     Cost function
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mapping-probabilities-to-classes">
     Mapping probabilities to classes
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-evaluation">
     Model evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiclass-logistic-regression">
   Multiclass logistic regression
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#procedure">
     Procedure
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#softmax-activation">
     Softmax activation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scikit-learn-example">
     Scikit-Learn example
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="logistic-regression">
<span id="id1"></span><h1>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">#</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id17">Introduction</a></p>
<ul>
<li><p><a class="reference internal" href="#comparison-to-linear-regression" id="id18">Comparison to linear regression</a></p></li>
<li><p><a class="reference internal" href="#types-of-logistic-regression" id="id19">Types of logistic regression</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#binary-logistic-regression" id="id20">Binary logistic regression</a></p>
<ul>
<li><p><a class="reference internal" href="#sigmoid-activation" id="id21">Sigmoid activation</a></p></li>
<li><p><a class="reference internal" href="#decision-boundary" id="id22">Decision boundary</a></p></li>
<li><p><a class="reference internal" href="#making-predictions" id="id23">Making predictions</a></p></li>
<li><p><a class="reference internal" href="#cost-function" id="id24">Cost function</a></p></li>
<li><p><a class="reference internal" href="#gradient-descent" id="id25">Gradient descent</a></p></li>
<li><p><a class="reference internal" href="#mapping-probabilities-to-classes" id="id26">Mapping probabilities to classes</a></p></li>
<li><p><a class="reference internal" href="#training" id="id27">Training</a></p></li>
<li><p><a class="reference internal" href="#model-evaluation" id="id28">Model evaluation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#multiclass-logistic-regression" id="id29">Multiclass logistic regression</a></p>
<ul>
<li><p><a class="reference internal" href="#procedure" id="id30">Procedure</a></p></li>
<li><p><a class="reference internal" href="#softmax-activation" id="id31">Softmax activation</a></p></li>
<li><p><a class="reference internal" href="#scikit-learn-example" id="id32">Scikit-Learn example</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="introduction">
<h2><a class="toc-backref" href="#id17">Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<p>Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Unlike linear regression which outputs continuous number values, logistic regression transforms its output using the logistic sigmoid function to return a probability value which can then be mapped to two or more discrete classes.</p>
<section id="comparison-to-linear-regression">
<h3><a class="toc-backref" href="#id18">Comparison to linear regression</a><a class="headerlink" href="#comparison-to-linear-regression" title="Permalink to this headline">#</a></h3>
<p>Given data on time spent studying and exam scores. <a class="reference internal" href="linear_regression.html"><span class="doc">Linear Regression</span></a> and logistic regression can predict different things:</p>
<blockquote>
<div><ul class="simple">
<li><p><strong>Linear Regression</strong> could help us predict the student‚Äôs test score on a scale of 0 - 100. Linear regression predictions are continuous (numbers in a range).</p></li>
<li><p><strong>Logistic Regression</strong> could help use predict whether the student passed or failed. Logistic regression predictions are discrete (only specific values or categories are allowed). We can also view probability scores underlying the model‚Äôs classifications.</p></li>
</ul>
</div></blockquote>
</section>
<section id="types-of-logistic-regression">
<h3><a class="toc-backref" href="#id19">Types of logistic regression</a><a class="headerlink" href="#types-of-logistic-regression" title="Permalink to this headline">#</a></h3>
<blockquote>
<div><ul class="simple">
<li><p>Binary (Pass/Fail)</p></li>
<li><p>Multi (Cats, Dogs, Sheep)</p></li>
<li><p>Ordinal (Low, Medium, High)</p></li>
</ul>
</div></blockquote>
</section>
</section>
<section id="binary-logistic-regression">
<h2><a class="toc-backref" href="#id20">Binary logistic regression</a><a class="headerlink" href="#binary-logistic-regression" title="Permalink to this headline">#</a></h2>
<p>Say we‚Äôre given <a class="reference external" href="http://scilab.io/wp-content/uploads/2016/07/data_classification.csv">data</a> on student exam results and our goal is to predict whether a student will pass or fail based on number of hours slept and hours spent studying. We have two features (hours slept, hours studied) and two classes: passed (1) and failed (0).</p>
<table class="table">
<colgroup>
<col style="width: 35%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><strong>Studied</strong></p></td>
<td><p><strong>Slept</strong></p></td>
<td><p><strong>Passed</strong></p></td>
</tr>
<tr class="row-even"><td><p>4.85</p></td>
<td><p>9.63</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>8.62</p></td>
<td><p>3.23</p></td>
<td><p>0</p></td>
</tr>
<tr class="row-even"><td><p>5.43</p></td>
<td><p>8.23</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>9.21</p></td>
<td><p>6.34</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<p>Graphically we could represent our data with a scatter plot.</p>
<img alt="_images/logistic_regression_exam_scores_scatter.png" class="align-center" src="_images/logistic_regression_exam_scores_scatter.png" />
<section id="sigmoid-activation">
<h3><a class="toc-backref" href="#id21">Sigmoid activation</a><a class="headerlink" href="#sigmoid-activation" title="Permalink to this headline">#</a></h3>
<p>In order to map predicted values to probabilities, we use the <a class="reference internal" href="activation_functions.html#activation-sigmoid"><span class="std std-ref">sigmoid</span></a> function. The function maps any real value into another value between 0 and 1. In machine learning, we use sigmoid to map predictions to probabilities.</p>
<p class="rubric">Math</p>
<div class="math notranslate nohighlight">
\[S(z) = \frac{1} {1 + e^{-z}}\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(s(z)\)</span> = output between 0 and 1 (probability estimate)</p></li>
<li><p><span class="math notranslate nohighlight">\(z\)</span> = input to the function (your algorithm‚Äôs prediction e.g. mx + b)</p></li>
<li><p><span class="math notranslate nohighlight">\(e\)</span> = base of natural log</p></li>
</ul>
</div>
<p class="rubric">Graph</p>
<img alt="_images/sigmoid.png" class="align-center" src="_images/sigmoid.png" />
<p class="rubric">Code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
<span class="linenos">2</span>  <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="decision-boundary">
<h3><a class="toc-backref" href="#id22">Decision boundary</a><a class="headerlink" href="#decision-boundary" title="Permalink to this headline">#</a></h3>
<p>Our current prediction function returns a probability score between 0 and 1. In order to map this to a discrete class (true/false, cat/dog), we select a threshold value or tipping point above which we will classify values into class 1 and below which we classify values into class 2.</p>
<div class="math notranslate nohighlight">
\[\begin{split}p \geq 0.5, class=1 \\
p &lt; 0.5, class=0\end{split}\]</div>
<p>For example, if our threshold was .5 and our prediction function returned .7, we would classify this observation as positive. If our prediction was .2 we would classify the observation as negative. For logistic regression with multiple classes we could select the class with the highest predicted probability.</p>
<img alt="_images/logistic_regression_sigmoid_w_threshold.png" class="align-center" src="_images/logistic_regression_sigmoid_w_threshold.png" />
</section>
<section id="making-predictions">
<h3><a class="toc-backref" href="#id23">Making predictions</a><a class="headerlink" href="#making-predictions" title="Permalink to this headline">#</a></h3>
<p>Using our knowledge of sigmoid functions and decision boundaries, we can now write a prediction function. A prediction function in logistic regression returns the probability of our observation being positive, True, or ‚ÄúYes‚Äù. We call this class 1 and its notation is <span class="math notranslate nohighlight">\(P(class=1)\)</span>. As the probability gets closer to 1, our model is more confident that the observation is in class 1.</p>
<p class="rubric">Math</p>
<p>Let‚Äôs use the same <a class="reference internal" href="linear_regression.html#multiple-linear-regression-predict"><span class="std std-ref">multiple linear regression</span></a> equation from our linear regression tutorial.</p>
<div class="math notranslate nohighlight">
\[z = W_0 + W_1 Studied + W_2 Slept\]</div>
<p>This time however we will transform the output using the sigmoid function to return a probability value between 0 and 1.</p>
<div class="math notranslate nohighlight">
\[P(class=1) = \frac{1} {1 + e^{-z}}\]</div>
<p>If the model returns .4 it believes there is only a 40% chance of passing. If our decision boundary was .5, we would categorize this observation as ‚ÄúFail.‚Äù‚Äù</p>
<p class="rubric">Code</p>
<p>We wrap the sigmoid function over the same prediction function we used in <a class="reference internal" href="linear_regression.html#multiple-linear-regression-predict"><span class="std std-ref">multiple linear regression</span></a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
<span class="linenos">2</span>    <span class="sd">&#39;&#39;&#39;</span>
<span class="linenos">3</span><span class="sd">    Returns 1D array of probabilities</span>
<span class="linenos">4</span><span class="sd">    that the class label == 1</span>
<span class="linenos">5</span><span class="sd">    &#39;&#39;&#39;</span>
<span class="linenos">6</span>    <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="linenos">7</span>    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="cost-function">
<h3><a class="toc-backref" href="#id24">Cost function</a><a class="headerlink" href="#cost-function" title="Permalink to this headline">#</a></h3>
<p>Unfortunately we can‚Äôt (or at least shouldn‚Äôt) use the same cost function <a class="reference internal" href="loss_functions.html#mse"><span class="std std-ref">MSE (L2)</span></a> as we did for linear regression. Why? There is a great math explanation in chapter 3 of Michael Neilson‚Äôs deep learning book <a class="footnote-reference brackets" href="#id12" id="id2">5</a>, but for now I‚Äôll simply say it‚Äôs because our prediction function is non-linear (due to sigmoid transform). Squaring this prediction as we do in MSE results in a non-convex function with many local minimums. If our cost function has many local minimums, gradient descent may not find the optimal global minimum.</p>
<p class="rubric">Math</p>
<p>Instead of Mean Squared Error, we use a cost function called <a class="reference internal" href="loss_functions.html#loss-cross-entropy"><span class="std std-ref">Cross-Entropy</span></a>, also known as Log Loss. Cross-entropy loss can be divided into two separate cost functions: one for <span class="math notranslate nohighlight">\(y=1\)</span> and one for <span class="math notranslate nohighlight">\(y=0\)</span>.</p>
<img alt="_images/ng_cost_function_logistic.png" class="align-center" src="_images/ng_cost_function_logistic.png" />
<p>The benefits of taking the logarithm reveal themselves when you look at the cost function graphs for y=1 and y=0. These smooth monotonic functions <a class="footnote-reference brackets" href="#id14" id="id3">7</a> (always increasing or always decreasing) make it easy to calculate the gradient and minimize cost. Image from Andrew Ng‚Äôs slides on logistic regression <a class="footnote-reference brackets" href="#id8" id="id4">1</a>.</p>
<img alt="_images/y1andy2_logistic_function.png" class="align-center" src="_images/y1andy2_logistic_function.png" />
<p>The key thing to note is the cost function penalizes confident and wrong predictions more than it rewards confident and right predictions! The corollary is increasing prediction accuracy (closer to 0 or 1) has diminishing returns on reducing cost due to the logistic nature of our cost function.</p>
<p class="rubric">Above functions compressed into one</p>
<img alt="_images/logistic_cost_function_joined.png" class="align-center" src="_images/logistic_cost_function_joined.png" />
<p>Multiplying by <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\((1-y)\)</span> in the above equation is a sneaky trick that let‚Äôs us use the same equation to solve for both y=1 and y=0 cases. If y=0, the first side cancels out. If y=1, the second side cancels out. In both cases we only perform the operation we need to perform.</p>
<p class="rubric">Vectorized cost function</p>
<img alt="_images/logistic_cost_function_vectorized.png" class="align-center" src="_images/logistic_cost_function_vectorized.png" />
<p class="rubric">Code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">def</span> <span class="nf">cost_function</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">):</span>
<span class="linenos"> 2</span>    <span class="sd">&#39;&#39;&#39;</span>
<span class="linenos"> 3</span><span class="sd">    Using Mean Absolute Error</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="sd">    Features:(100,3)</span>
<span class="linenos"> 6</span><span class="sd">    Labels: (100,1)</span>
<span class="linenos"> 7</span><span class="sd">    Weights:(3,1)</span>
<span class="linenos"> 8</span><span class="sd">    Returns 1D matrix of predictions</span>
<span class="linenos"> 9</span><span class="sd">    Cost = (labels*log(predictions) + (1-labels)*log(1-predictions) ) / len(labels)</span>
<span class="linenos">10</span><span class="sd">    &#39;&#39;&#39;</span>
<span class="linenos">11</span>    <span class="n">observations</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="linenos">12</span>
<span class="linenos">13</span>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span>    <span class="c1">#Take the error when label=1</span>
<span class="linenos">16</span>    <span class="n">class1_cost</span> <span class="o">=</span> <span class="o">-</span><span class="n">labels</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span>
<span class="linenos">17</span>
<span class="linenos">18</span>    <span class="c1">#Take the error when label=0</span>
<span class="linenos">19</span>    <span class="n">class2_cost</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">labels</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">predictions</span><span class="p">)</span>
<span class="linenos">20</span>
<span class="linenos">21</span>    <span class="c1">#Take the sum of both costs</span>
<span class="linenos">22</span>    <span class="n">cost</span> <span class="o">=</span> <span class="n">class1_cost</span> <span class="o">-</span> <span class="n">class2_cost</span>
<span class="linenos">23</span>
<span class="linenos">24</span>    <span class="c1">#Take the average cost</span>
<span class="linenos">25</span>    <span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">observations</span>
<span class="linenos">26</span>
<span class="linenos">27</span>    <span class="k">return</span> <span class="n">cost</span>
</pre></div>
</div>
</section>
<section id="gradient-descent">
<h3><a class="toc-backref" href="#id25">Gradient descent</a><a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h3>
<p>To minimize our cost, we use <a class="reference internal" href="gradient_descent.html"><span class="doc">Gradient Descent</span></a> just like before in <a class="reference internal" href="linear_regression.html"><span class="doc">Linear Regression</span></a>. There are other more sophisticated optimization algorithms out there such as conjugate gradient like <a class="reference internal" href="optimizers.html#optimizers-lbfgs"><span class="std std-ref">BFGS</span></a>, but you don‚Äôt have to worry about these. Machine learning libraries like Scikit-learn hide their implementations so you can focus on more interesting things!</p>
<p class="rubric">Math</p>
<p>One of the neat properties of the sigmoid function is its derivative is easy to calculate. If you‚Äôre curious, there is a good walk-through derivation on stack overflow <a class="footnote-reference brackets" href="#id13" id="id5">6</a>. Michael Neilson also covers the topic in chapter 3 of his book.</p>
<div class="math notranslate nohighlight">
\[\begin{align}
s'(z) &amp; = s(z)(1 - s(z))
\end{align}\]</div>
<p>Which leads to an equally beautiful and convenient cost function derivative:</p>
<div class="math notranslate nohighlight">
\[C' = x(s(z) - y)\]</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(C'\)</span> is the derivative of cost with respect to weights</p></li>
<li><p><span class="math notranslate nohighlight">\(y\)</span> is the actual class label (0 or 1)</p></li>
<li><p><span class="math notranslate nohighlight">\(s(z)\)</span> is your model‚Äôs prediction</p></li>
<li><p><span class="math notranslate nohighlight">\(x\)</span> is your feature or feature vector.</p></li>
</ul>
</div>
<p>Notice how this gradient is the same as the <a class="reference internal" href="loss_functions.html#mse"><span class="std std-ref">MSE (L2)</span></a> gradient, the only difference is the hypothesis function.</p>
<p class="rubric">Pseudocode</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Repeat</span> <span class="p">{</span>

 <span class="mf">1.</span> <span class="n">Calculate</span> <span class="n">gradient</span> <span class="n">average</span>
 <span class="mf">2.</span> <span class="n">Multiply</span> <span class="n">by</span> <span class="n">learning</span> <span class="n">rate</span>
 <span class="mf">3.</span> <span class="n">Subtract</span> <span class="kn">from</span> <span class="nn">weights</span>

<span class="p">}</span>
</pre></div>
</div>
<p class="rubric">Code</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">):</span>
<span class="linenos"> 2</span>    <span class="sd">&#39;&#39;&#39;</span>
<span class="linenos"> 3</span><span class="sd">    Vectorized Gradient Descent</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="sd">    Features:(200, 3)</span>
<span class="linenos"> 6</span><span class="sd">    Labels: (200, 1)</span>
<span class="linenos"> 7</span><span class="sd">    Weights:(3, 1)</span>
<span class="linenos"> 8</span><span class="sd">    &#39;&#39;&#39;</span>
<span class="linenos"> 9</span>    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span>    <span class="c1">#1 - Get Predictions</span>
<span class="linenos">12</span>    <span class="n">predictions</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="linenos">13</span>
<span class="linenos">14</span>    <span class="c1">#2 Transpose features from (200, 3) to (3, 200)</span>
<span class="linenos">15</span>    <span class="c1"># So we can multiply w the (200,1)  cost matrix.</span>
<span class="linenos">16</span>    <span class="c1"># Returns a (3,1) matrix holding 3 partial derivatives --</span>
<span class="linenos">17</span>    <span class="c1"># one for each feature -- representing the aggregate</span>
<span class="linenos">18</span>    <span class="c1"># slope of the cost function across all observations</span>
<span class="linenos">19</span>    <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>  <span class="n">predictions</span> <span class="o">-</span> <span class="n">labels</span><span class="p">)</span>
<span class="linenos">20</span>
<span class="linenos">21</span>    <span class="c1">#3 Take the average cost derivative for each feature</span>
<span class="linenos">22</span>    <span class="n">gradient</span> <span class="o">/=</span> <span class="n">N</span>
<span class="linenos">23</span>
<span class="linenos">24</span>    <span class="c1">#4 - Multiply the gradient by our learning rate</span>
<span class="linenos">25</span>    <span class="n">gradient</span> <span class="o">*=</span> <span class="n">lr</span>
<span class="linenos">26</span>
<span class="linenos">27</span>    <span class="c1">#5 - Subtract from our weights to minimize cost</span>
<span class="linenos">28</span>    <span class="n">weights</span> <span class="o">-=</span> <span class="n">gradient</span>
<span class="linenos">29</span>
<span class="linenos">30</span>    <span class="k">return</span> <span class="n">weights</span>
</pre></div>
</div>
</section>
<section id="mapping-probabilities-to-classes">
<h3><a class="toc-backref" href="#id26">Mapping probabilities to classes</a><a class="headerlink" href="#mapping-probabilities-to-classes" title="Permalink to this headline">#</a></h3>
<p>The final step is assign class labels (0 or 1) to our predicted probabilities.</p>
<p class="rubric">Decision boundary</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="k">def</span> <span class="nf">decision_boundary</span><span class="p">(</span><span class="n">prob</span><span class="p">):</span>
<span class="linenos">2</span>    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">prob</span> <span class="o">&gt;=</span> <span class="mf">.5</span> <span class="k">else</span> <span class="mi">0</span>
</pre></div>
</div>
<p class="rubric">Convert probabilities to classes</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="k">def</span> <span class="nf">classify</span><span class="p">(</span><span class="n">predictions</span><span class="p">):</span>
<span class="linenos">2</span>    <span class="sd">&#39;&#39;&#39;</span>
<span class="linenos">3</span><span class="sd">    input  - N element array of predictions between 0 and 1</span>
<span class="linenos">4</span><span class="sd">    output - N element array of 0s (False) and 1s (True)</span>
<span class="linenos">5</span><span class="sd">    &#39;&#39;&#39;</span>
<span class="linenos">6</span>    <span class="n">decision_boundary</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">decision_boundary</span><span class="p">)</span>
<span class="linenos">7</span>    <span class="k">return</span> <span class="n">decision_boundary</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric">Example output</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Probabilities</span> <span class="o">=</span> <span class="p">[</span> <span class="mf">0.967</span><span class="p">,</span> <span class="mf">0.448</span><span class="p">,</span> <span class="mf">0.015</span><span class="p">,</span> <span class="mf">0.780</span><span class="p">,</span> <span class="mf">0.978</span><span class="p">,</span> <span class="mf">0.004</span><span class="p">]</span>
<span class="n">Classifications</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="training">
<h3><a class="toc-backref" href="#id27">Training</a><a class="headerlink" href="#training" title="Permalink to this headline">#</a></h3>
<p>Our training code is the same as we used for <a class="reference internal" href="linear_regression.html#simple-linear-regression-training"><span class="std std-ref">linear regression</span></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">iters</span><span class="p">):</span>
<span class="linenos"> 2</span>    <span class="n">cost_history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="linenos"> 3</span>
<span class="linenos"> 4</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
<span class="linenos"> 5</span>        <span class="n">weights</span> <span class="o">=</span> <span class="n">update_weights</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">lr</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span>        <span class="c1">#Calculate error for auditing purposes</span>
<span class="linenos"> 8</span>        <span class="n">cost</span> <span class="o">=</span> <span class="n">cost_function</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
<span class="linenos"> 9</span>        <span class="n">cost_history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
<span class="linenos">10</span>
<span class="linenos">11</span>        <span class="c1"># Log Progress</span>
<span class="linenos">12</span>        <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">1000</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
<span class="linenos">13</span>            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;iter: &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; cost: &quot;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">cost</span><span class="p">))</span>
<span class="linenos">14</span>
<span class="linenos">15</span>    <span class="k">return</span> <span class="n">weights</span><span class="p">,</span> <span class="n">cost_history</span>
</pre></div>
</div>
</section>
<section id="model-evaluation">
<h3><a class="toc-backref" href="#id28">Model evaluation</a><a class="headerlink" href="#model-evaluation" title="Permalink to this headline">#</a></h3>
<p>If our model is working, we should see our cost decrease after every iteration.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">iter</span><span class="p">:</span> <span class="mi">0</span> <span class="n">cost</span><span class="p">:</span> <span class="mf">0.635</span>
<span class="nb">iter</span><span class="p">:</span> <span class="mi">1000</span> <span class="n">cost</span><span class="p">:</span> <span class="mf">0.302</span>
<span class="nb">iter</span><span class="p">:</span> <span class="mi">2000</span> <span class="n">cost</span><span class="p">:</span> <span class="mf">0.264</span>
</pre></div>
</div>
<p><strong>Final cost:</strong>  0.2487.  <strong>Final weights:</strong> [-8.197, .921, .738]</p>
<p class="rubric">Cost history</p>
<img alt="_images/logistic_regression_loss_history.png" class="align-center" src="_images/logistic_regression_loss_history.png" />
<p class="rubric">Accuracy</p>
<p><a class="reference internal" href="glossary.html#glossary-accuracy"><span class="std std-ref">Accuracy</span></a> measures how correct our predictions were. In this case we simply compare predicted labels to true labels and divide by the total.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos">1</span><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">predicted_labels</span><span class="p">,</span> <span class="n">actual_labels</span><span class="p">):</span>
<span class="linenos">2</span>    <span class="n">diff</span> <span class="o">=</span> <span class="n">predicted_labels</span> <span class="o">-</span> <span class="n">actual_labels</span>
<span class="linenos">3</span>    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span>
</pre></div>
</div>
<p class="rubric">Decision boundary</p>
<p>Another helpful technique is to plot the decision boundary on top of our predictions to see how our labels compare to the actual labels. This involves plotting our predicted probabilities and coloring them with their true labels.</p>
<img alt="_images/logistic_regression_final_decision_boundary.png" class="align-center" src="_images/logistic_regression_final_decision_boundary.png" />
<p class="rubric">Code to plot the decision boundary</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">trues</span><span class="p">,</span> <span class="n">falses</span><span class="p">):</span>
<span class="linenos"> 2</span>    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="linenos"> 3</span>    <span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span>    <span class="n">no_of_preds</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trues</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">falses</span><span class="p">)</span>
<span class="linenos"> 6</span>
<span class="linenos"> 7</span>    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trues</span><span class="p">))],</span> <span class="n">trues</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Trues&#39;</span><span class="p">)</span>
<span class="linenos"> 8</span>    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">falses</span><span class="p">))],</span> <span class="n">falses</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Falses&#39;</span><span class="p">)</span>
<span class="linenos"> 9</span>
<span class="linenos">10</span>    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">);</span>
<span class="linenos">11</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Decision Boundary&quot;</span><span class="p">)</span>
<span class="linenos">12</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;N/2&#39;</span><span class="p">)</span>
<span class="linenos">13</span>    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted Probability&#39;</span><span class="p">)</span>
<span class="linenos">14</span>    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mf">.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">)</span>
<span class="linenos">15</span>    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="multiclass-logistic-regression">
<h2><a class="toc-backref" href="#id29">Multiclass logistic regression</a><a class="headerlink" href="#multiclass-logistic-regression" title="Permalink to this headline">#</a></h2>
<p>Instead of <span class="math notranslate nohighlight">\(y = {0,1}\)</span> we will expand our definition so that <span class="math notranslate nohighlight">\(y = {0,1...n}\)</span>. Basically we re-run binary classification multiple times, once for each class.</p>
<section id="procedure">
<h3><a class="toc-backref" href="#id30">Procedure</a><a class="headerlink" href="#procedure" title="Permalink to this headline">#</a></h3>
<blockquote>
<div><ol class="arabic simple">
<li><p>Divide the problem into n+1 binary classification problems (+1 because the index starts at 0?).</p></li>
<li><p>For each class‚Ä¶</p></li>
<li><p>Predict the probability the observations are in that single class.</p></li>
<li><p>prediction = &lt;math&gt;max(probability of the classes)</p></li>
</ol>
</div></blockquote>
<p>For each sub-problem, we select one class (YES) and lump all the others into a second class (NO). Then we take the class with the highest predicted value.</p>
</section>
<section id="softmax-activation">
<h3><a class="toc-backref" href="#id31">Softmax activation</a><a class="headerlink" href="#softmax-activation" title="Permalink to this headline">#</a></h3>
<p>The softmax function (softargmax or normalized exponential function) is a function that takes as input a vector of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, each component will be in the interval [ 0 , 1 ] , and the components will add up to 1, so that they can be interpreted as probabilities.
The standard (unit) softmax function is defined by the formula</p>
<div class="math notranslate nohighlight">
\[\begin{align}
 œÉ(z_i) = \frac{e^{z_{(i)}}}{\sum_{j=1}^K e^{z_{(j)}}}\ \ \ for\ i=1,.,.,.,K\ and\ z=z_1,.,.,.,z_K
\end{align}\]</div>
<p>In words: we apply the standard exponential function to each element <span class="math notranslate nohighlight">\(z_i\)</span> of the input vector <span class="math notranslate nohighlight">\(z\)</span> and normalize these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector <span class="math notranslate nohighlight">\(œÉ(z)\)</span> is 1. <a class="footnote-reference brackets" href="#id16" id="id6">9</a></p>
</section>
<section id="scikit-learn-example">
<h3><a class="toc-backref" href="#id32">Scikit-Learn example</a><a class="headerlink" href="#scikit-learn-example" title="Permalink to this headline">#</a></h3>
<p>Let‚Äôs compare our performance to the <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model provided by scikit-learn <a class="footnote-reference brackets" href="#id15" id="id7">8</a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="linenos"> 1</span><span class="kn">import</span> <span class="nn">sklearn</span>
<span class="linenos"> 2</span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="linenos"> 3</span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="linenos"> 4</span>
<span class="linenos"> 5</span><span class="c1"># Normalize grades to values between 0 and 1 for more efficient computation</span>
<span class="linenos"> 6</span><span class="n">normalized_range</span> <span class="o">=</span> <span class="n">sklearn</span><span class="o">.</span><span class="n">preprocessing</span><span class="o">.</span><span class="n">MinMaxScaler</span><span class="p">(</span><span class="n">feature_range</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="linenos"> 7</span>
<span class="linenos"> 8</span><span class="c1"># Extract Features + Labels</span>
<span class="linenos"> 9</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span> <span class="o">=</span>  <span class="p">(</span><span class="mi">100</span><span class="p">,)</span> <span class="c1">#scikit expects this</span>
<span class="linenos">10</span><span class="n">features</span> <span class="o">=</span> <span class="n">normalized_range</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="linenos">11</span>
<span class="linenos">12</span><span class="c1"># Create Test/Train</span>
<span class="linenos">13</span><span class="n">features_train</span><span class="p">,</span><span class="n">features_test</span><span class="p">,</span><span class="n">labels_train</span><span class="p">,</span><span class="n">labels_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">labels</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>
<span class="linenos">14</span>
<span class="linenos">15</span><span class="c1"># Scikit Logistic Regression</span>
<span class="linenos">16</span><span class="n">scikit_log_reg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="linenos">17</span><span class="n">scikit_log_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features_train</span><span class="p">,</span><span class="n">labels_train</span><span class="p">)</span>
<span class="linenos">18</span>
<span class="linenos">19</span><span class="c1">#Score is Mean Accuracy</span>
<span class="linenos">20</span><span class="n">scikit_score</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">features_test</span><span class="p">,</span><span class="n">labels_test</span><span class="p">)</span>
<span class="linenos">21</span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Scikit score: &#39;</span><span class="p">,</span><span class="n">scikit_score</span><span class="p">)</span>
<span class="linenos">22</span>
<span class="linenos">23</span><span class="c1">#Our Mean Accuracy</span>
<span class="linenos">24</span><span class="n">observations</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="n">run</span><span class="p">()</span>
<span class="linenos">25</span><span class="n">probabilities</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
<span class="linenos">26</span><span class="n">classifications</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">probabilities</span><span class="p">)</span>
<span class="linenos">27</span><span class="n">our_acc</span> <span class="o">=</span> <span class="n">accuracy</span><span class="p">(</span><span class="n">classifications</span><span class="p">,</span><span class="n">labels</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
<span class="linenos">28</span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Our score: &#39;</span><span class="p">,</span><span class="n">our_acc</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Scikit score:</strong>  0.88. <strong>Our score:</strong> 0.89</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p><a class="reference external" href="http://www.holehouse.org/mlclass/06_Logistic_Regression.html">http://www.holehouse.org/mlclass/06_Logistic_Regression.html</a></p>
</dd>
<dt class="label" id="id9"><span class="brackets">2</span></dt>
<dd><p><a class="reference external" href="http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning">http://machinelearningmastery.com/logistic-regression-tutorial-for-machine-learning</a></p>
</dd>
<dt class="label" id="id10"><span class="brackets">3</span></dt>
<dd><p><a class="reference external" href="https://scilab.io/machine-learning-logistic-regression-tutorial/">https://scilab.io/machine-learning-logistic-regression-tutorial/</a></p>
</dd>
<dt class="label" id="id11"><span class="brackets">4</span></dt>
<dd><p><a class="reference external" href="https://github.com/perborgen/LogisticRegression/blob/master/logistic.py">https://github.com/perborgen/LogisticRegression/blob/master/logistic.py</a></p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id2">5</a></span></dt>
<dd><p><a class="reference external" href="http://neuralnetworksanddeeplearning.com/chap3.html">http://neuralnetworksanddeeplearning.com/chap3.html</a></p>
</dd>
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p><a class="reference external" href="http://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x">http://math.stackexchange.com/questions/78575/derivative-of-sigmoid-function-sigma-x-frac11e-x</a></p>
</dd>
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id3">7</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Monotoniconotonic_function">https://en.wikipedia.org/wiki/Monotoniconotonic_function</a></p>
</dd>
<dt class="label" id="id15"><span class="brackets"><a class="fn-backref" href="#id7">8</a></span></dt>
<dd><p><a class="reference external" href="http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression">http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression</a>&gt;</p>
</dd>
<dt class="label" id="id16"><span class="brackets"><a class="fn-backref" href="#id6">9</a></span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Softmax_function">https://en.wikipedia.org/wiki/Softmax_function</a></p>
</dd>
</dl>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="gradient_descent.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Gradient Descent</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="glossary.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Glossary</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Team<br/>
  
      &copy; Copyright 2017.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>