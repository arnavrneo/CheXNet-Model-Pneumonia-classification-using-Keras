
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Papers &#8212; üè† ML Glossary</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/theme_overrides.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Other Content" href="other_content.html" />
    <link rel="prev" title="Libraries" href="libraries.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"><br/>
<a href="https://github.com/bfortuner/ml-glossary">
    
    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 496 512"><!-- Font Awesome Pro 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    Edit on GitHub
</a></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">üè† ML Glossary</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="linear_regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient_descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic_regression.html">
   Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   Glossary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="calculus.html">
   Calculus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability.html">
   Probability (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statistics.html">
   Statistics (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="math_notation.html">
   Notation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nn_concepts.html">
   Concepts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="forwardpropagation.html">
   Forwardpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backpropagation.html">
   Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="activation_functions.html">
   Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loss_functions.html">
   Loss Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optimizers.html">
   Optimizers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regularization.html">
   Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="architectures.html">
   Architectures
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Algorithms (TODO)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="classification_algos.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="clustering_algos.html">
   Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_algos.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="reinforcement_learning.html">
   Reinforcement Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="datasets.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="libraries.html">
   Libraries
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Papers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="other_content.html">
   Other
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="contribute.html">
   How to contribute
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/papers.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning">
   Machine Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning">
   Deep Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#understanding">
     Understanding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization-training-techniques">
     Optimization / Training Techniques
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-generative-models">
     Unsupervised / Generative Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-segmentation-object-detection">
     Image Segmentation / Object Detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-video">
     Image / Video
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#natural-language-processing">
     Natural Language Processing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#speech-other">
     Speech / Other
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reinforcement-learning">
     Reinforcement Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#new-papers">
     New papers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classic-papers">
     Classic Papers
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Papers</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#machine-learning">
   Machine Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-learning">
   Deep Learning
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#understanding">
     Understanding
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optimization-training-techniques">
     Optimization / Training Techniques
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#unsupervised-generative-models">
     Unsupervised / Generative Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-segmentation-object-detection">
     Image Segmentation / Object Detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-video">
     Image / Video
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#natural-language-processing">
     Natural Language Processing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#speech-other">
     Speech / Other
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reinforcement-learning">
     Reinforcement Learning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#new-papers">
     New papers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#classic-papers">
     Classic Papers
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="papers">
<span id="id1"></span><h1>Papers<a class="headerlink" href="#papers" title="Permalink to this headline">#</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#machine-learning" id="id2">Machine Learning</a></p></li>
<li><p><a class="reference internal" href="#deep-learning" id="id3">Deep Learning</a></p>
<ul>
<li><p><a class="reference internal" href="#understanding" id="id4">Understanding</a></p></li>
<li><p><a class="reference internal" href="#optimization-training-techniques" id="id5">Optimization / Training Techniques</a></p></li>
<li><p><a class="reference internal" href="#unsupervised-generative-models" id="id6">Unsupervised / Generative Models</a></p></li>
<li><p><a class="reference internal" href="#image-segmentation-object-detection" id="id7">Image Segmentation / Object Detection</a></p></li>
<li><p><a class="reference internal" href="#image-video" id="id8">Image / Video</a></p></li>
<li><p><a class="reference internal" href="#natural-language-processing" id="id9">Natural Language Processing</a></p></li>
<li><p><a class="reference internal" href="#speech-other" id="id10">Speech / Other</a></p></li>
<li><p><a class="reference internal" href="#reinforcement-learning" id="id11">Reinforcement Learning</a></p></li>
<li><p><a class="reference internal" href="#new-papers" id="id12">New papers</a></p></li>
<li><p><a class="reference internal" href="#classic-papers" id="id13">Classic Papers</a></p></li>
</ul>
</li>
</ul>
</div>
<section id="machine-learning">
<h2><a class="toc-backref" href="#id2">Machine Learning</a><a class="headerlink" href="#machine-learning" title="Permalink to this headline">#</a></h2>
<p>Be the first to <a class="reference external" href="https://github.com/bfortuner/ml-cheatsheet">contribute!</a></p>
</section>
<section id="deep-learning">
<h2><a class="toc-backref" href="#id3">Deep Learning</a><a class="headerlink" href="#deep-learning" title="Permalink to this headline">#</a></h2>
<p>Forked from terryum‚Äôs <a class="reference external" href="https://github.com/terryum/awesome-deep-learning-papers">awesome deep learning papers</a>.</p>
<section id="understanding">
<h3><a class="toc-backref" href="#id4">Understanding</a><a class="headerlink" href="#understanding" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Distilling the knowledge in a neural network (2015), G. Hinton et al. <a class="reference external" href="http://arxiv.org/1503.02531">[pdf]</a></p></li>
<li><p>Deep neural networks are easily fooled: High confidence predictions for unrecognizable images (2015), A. Nguyen et al. <a class="reference external" href="http://arxiv.org/1412.1897">[pdf]</a></p></li>
<li><p>How transferable are features in deep neural networks? (2014), J. Yosinski et al. <a class="reference external" href="http://papers.nips.cc/paper/5347-how-transferable-are-features-in-deep-neural-networks.pdf">[pdf]</a></p></li>
<li><p>CNN features off-the-Shelf: An astounding baseline for recognition (2014), A. Razavian et al. <a class="reference external" href="http://www.cv-foundation.org//openaccess/content_cvpr_workshops_2014/W15/papers/Razavian_CNN_Features_Off-the-Shelf_2014_CVPR_paper.pdf">[pdf]</a></p></li>
<li><p>Learning and transferring mid-Level image representations using convolutional neural networks (2014), M. Oquab et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf">[pdf]</a></p></li>
<li><p>Visualizing and understanding convolutional networks (2014), M. Zeiler and R. Fergus <a class="reference external" href="http://arxiv.org/1311.2901">[pdf]</a></p></li>
<li><p>Decaf: A deep convolutional activation feature for generic visual recognition (2014), J. Donahue et al. <a class="reference external" href="http://arxiv.org/1310.1531">[pdf]</a></p></li>
</ul>
</section>
<section id="optimization-training-techniques">
<h3><a class="toc-backref" href="#id5">Optimization / Training Techniques</a><a class="headerlink" href="#optimization-training-techniques" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Batch normalization: Accelerating deep network training by reducing internal covariate shift (2015), S. Loffe and C. Szegedy <a class="reference external" href="http://arxiv.org/1502.03167">[pdf]</a></p></li>
<li><p>Delving deep into rectifiers: Surpassing human-level performance on imagenet classification (2015), K. He et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/He_Delving_Deep_into_ICCV_2015_paper.pdf">[pdf]</a></p></li>
<li><p>Dropout: A simple way to prevent neural networks from overfitting (2014), N. Srivastava et al. <a class="reference external" href="http://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">[pdf]</a></p></li>
<li><p>Adam: A method for stochastic optimization (2014), D. Kingma and J. Ba <a class="reference external" href="http://arxiv.org/1412.6980">[pdf]</a></p></li>
<li><p>Improving neural networks by preventing co-adaptation of feature detectors (2012), G. Hinton et al. <a class="reference external" href="http://arxiv.org/1207.0580.pdf">[pdf]</a></p></li>
<li><p>Random search for hyper-parameter optimization (2012) J. Bergstra and Y. Bengio <a class="reference external" href="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a">[pdf]</a></p></li>
</ul>
</section>
<section id="unsupervised-generative-models">
<h3><a class="toc-backref" href="#id6">Unsupervised / Generative Models</a><a class="headerlink" href="#unsupervised-generative-models" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Pixel recurrent neural networks (2016), A. Oord et al. <a class="reference external" href="http://arxiv.org/1601.06759v2.pdf">[pdf]</a></p></li>
<li><p>Improved techniques for training GANs (2016), T. Salimans et al. <a class="reference external" href="http://papers.nips.cc/paper/6125-improved-techniques-for-training-gans.pdf">[pdf]</a></p></li>
<li><p>Unsupervised representation learning with deep convolutional generative adversarial networks (2015), A. Radford et al. <a class="reference external" href="https://arxiv.org/1511.06434v2">[pdf]</a></p></li>
<li><p>DRAW: A recurrent neural network for image generation (2015), K. Gregor et al. <a class="reference external" href="http://arxiv.org/1502.04623">[pdf]</a></p></li>
<li><p>Generative adversarial nets (2014), I. Goodfellow et al. <a class="reference external" href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">[pdf]</a></p></li>
<li><p>Auto-encoding variational Bayes (2013), D. Kingma and M. Welling <a class="reference external" href="http://arxiv.org/1312.6114">[pdf]</a></p></li>
<li><p>Building high-level features using large scale unsupervised learning (2013), Q. Le et al. <a class="reference external" href="http://arxiv.org/1112.6209">[pdf]</a></p></li>
</ul>
</section>
<section id="image-segmentation-object-detection">
<h3><a class="toc-backref" href="#id7">Image Segmentation / Object Detection</a><a class="headerlink" href="#image-segmentation-object-detection" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>You only look once: Unified, real-time object detection (2016), J. Redmon et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf">[pdf]</a></p></li>
<li><p>Fully convolutional networks for semantic segmentation (2015), J. Long et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">[pdf]</a></p></li>
<li><p>Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks (2015), S. Ren et al. <a class="reference external" href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">[pdf]</a></p></li>
<li><p>Fast R-CNN (2015), R. Girshick <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">[pdf]</a></p></li>
<li><p>Rich feature hierarchies for accurate object detection and semantic segmentation (2014), R. Girshick et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf">[pdf]</a></p></li>
<li><p>Semantic image segmentation with deep convolutional nets and fully connected CRFs, L. Chen et al. <a class="reference external" href="https://arxiv.org/1412.7062">[pdf]</a></p></li>
<li><p>Learning hierarchical features for scene labeling (2013), C. Farabet et al. <a class="reference external" href="https://hal-enpc.archives-ouvertes.fr/docs/00/74/20/77/farabet-pami-13.pdf">[pdf]</a></p></li>
</ul>
</section>
<section id="image-video">
<h3><a class="toc-backref" href="#id8">Image / Video</a><a class="headerlink" href="#image-video" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Image Super-Resolution Using Deep Convolutional Networks (2016), C. Dong et al. <a class="reference external" href="https://arxiv.org/1501.00092v3.pdf">[pdf]</a></p></li>
<li><p>A neural algorithm of artistic style (2015), L. Gatys et al. <a class="reference external" href="https://arxiv.org/1508.06576">[pdf]</a></p></li>
<li><p>Deep visual-semantic alignments for generating image descriptions (2015), A. Karpathy and L. Fei-Fei <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf">[pdf]</a></p></li>
<li><p>Show, attend and tell: Neural image caption generation with visual attention (2015), K. Xu et al. <a class="reference external" href="http://arxiv.org/1502.03044">[pdf]</a></p></li>
<li><p>Show and tell: A neural image caption generator (2015), O. Vinyals et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">[pdf]</a></p></li>
<li><p>Long-term recurrent convolutional networks for visual recognition and description (2015), J. Donahue et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf">[pdf]</a></p></li>
<li><p>VQA: Visual question answering (2015), S. Antol et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf">[pdf]</a></p></li>
<li><p>DeepFace: Closing the gap to human-level performance in face verification (2014), Y. Taigman et al. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf">[pdf]</a>:</p></li>
<li><p>Large-scale video classification with convolutional neural networks (2014), A. Karpathy et al. <a class="reference external" href="http://vision.stanford.edu/karpathy14.pdf">[pdf]</a></p></li>
<li><p>DeepPose: Human pose estimation via deep neural networks (2014), A. Toshev and C. Szegedy <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.pdf">[pdf]</a></p></li>
<li><p>Two-stream convolutional networks for action recognition in videos (2014), K. Simonyan et al. <a class="reference external" href="http://papers.nips.cc/paper/5353-two-stream-convolutional-networks-for-action-recognition-in-videos.pdf">[pdf]</a></p></li>
<li><p>3D convolutional neural networks for human action recognition (2013), S. Ji et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/icml2010_JiXYY10.pdf">[pdf]</a></p></li>
</ul>
</section>
<section id="natural-language-processing">
<h3><a class="toc-backref" href="#id9">Natural Language Processing</a><a class="headerlink" href="#natural-language-processing" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Neural Architectures for Named Entity Recognition (2016), G. Lample et al. <a class="reference external" href="http://aclweb.org/anthology/N/N16/N16-1030.pdf">[pdf]</a></p></li>
<li><p>Exploring the limits of language modeling (2016), R. Jozefowicz et al. <a class="reference external" href="http://arxiv.org/1602.02410">[pdf]</a></p></li>
<li><p>Teaching machines to read and comprehend (2015), K. Hermann et al. <a class="reference external" href="http://papers.nips.cc/paper/5945-teaching-machines-to-read-and-comprehend.pdf">[pdf]</a></p></li>
<li><p>Effective approaches to attention-based neural machine translation (2015), M. Luong et al. <a class="reference external" href="https://arxiv.org/1508.04025">[pdf]</a></p></li>
<li><p>Conditional random fields as recurrent neural networks (2015), S. Zheng and S. Jayasumana. <a class="reference external" href="http://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Conditional_Random_Fields_ICCV_2015_paper.pdf">[pdf]</a></p></li>
<li><p>Memory networks (2014), J. Weston et al. <a class="reference external" href="https://arxiv.org/1410.3916">[pdf]</a></p></li>
<li><p>Neural turing machines (2014), A. Graves et al. <a class="reference external" href="https://arxiv.org/1410.5401">[pdf]</a></p></li>
<li><p>Neural machine translation by jointly learning to align and translate (2014), D. Bahdanau et al. <a class="reference external" href="http://arxiv.org/1409.0473">[pdf]</a></p></li>
<li><p>Sequence to sequence learning with neural networks (2014), I. Sutskever et al. <a class="reference external" href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">[pdf]</a></p></li>
<li><p>Learning phrase representations using RNN encoder-decoder for statistical machine translation (2014), K. Cho et al. <a class="reference external" href="http://arxiv.org/1406.1078">[pdf]</a></p></li>
<li><p>A convolutional neural network for modeling sentences (2014), N. Kalchbrenner et al. <a class="reference external" href="http://arxiv.org/1404.2188v1">[pdf]</a></p></li>
<li><p>Convolutional neural networks for sentence classification (2014), Y. Kim <a class="reference external" href="http://arxiv.org/1408.5882">[pdf]</a></p></li>
<li><p>Glove: Global vectors for word representation (2014), J. Pennington et al. <a class="reference external" href="http://anthology.aclweb.org/D/D14/D14-1162.pdf">[pdf]</a></p></li>
<li><p>Distributed representations of sentences and documents (2014), Q. Le and T. Mikolov <a class="reference external" href="http://arxiv.org/1405.4053">[pdf]</a></p></li>
<li><p>Distributed representations of words and phrases and their compositionality (2013), T. Mikolov et al. <a class="reference external" href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">[pdf]</a></p></li>
<li><p>Efficient estimation of word representations in vector space (2013), T. Mikolov et al.  <a class="reference external" href="http://arxiv.org/1301.3781">[pdf]</a></p></li>
<li><p>Recursive deep models for semantic compositionality over a sentiment treebank (2013), R. Socher et al. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;rep=rep1&amp;type=pdf">[pdf]</a></p></li>
<li><p>Generating sequences with recurrent neural networks (2013), A. Graves. <a class="reference external" href="https://arxiv.org/1308.0850">[pdf]</a></p></li>
</ul>
</section>
<section id="speech-other">
<h3><a class="toc-backref" href="#id10">Speech / Other</a><a class="headerlink" href="#speech-other" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>End-to-end attention-based large vocabulary speech recognition (2016), D. Bahdanau et al. <a class="reference external" href="https://arxiv.org/1508.04395">[pdf]</a></p></li>
<li><p>Deep speech 2: End-to-end speech recognition in English and Mandarin (2015), D. Amodei et al. <a class="reference external" href="https://arxiv.org/1512.02595">[pdf]</a></p></li>
<li><p>Speech recognition with deep recurrent neural networks (2013), A. Graves <a class="reference external" href="http://arxiv.org/1303.5778.pdf">[pdf]</a></p></li>
<li><p>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups (2012), G. Hinton et al. <a class="reference external" href="http://www.cs.toronto.edu/~asamir/papers/SPM_DNN_12.pdf">[pdf]</a></p></li>
<li><p>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition (2012) G. Dahl et al. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.337.7548&amp;rep=rep1&amp;type=pdf">[pdf]</a></p></li>
<li><p>Acoustic modeling using deep belief networks (2012), A. Mohamed et al. <a class="reference external" href="http://www.cs.toronto.edu/~asamir/papers/speechDBN_jrnl.pdf">[pdf]</a></p></li>
</ul>
</section>
<section id="reinforcement-learning">
<h3><a class="toc-backref" href="#id11">Reinforcement Learning</a><a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>End-to-end training of deep visuomotor policies (2016), S. Levine et al. <a class="reference external" href="http://www.jmlr.org/papers/volume17/15-522/source/15-522.pdf">[pdf]</a></p></li>
<li><p>Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection (2016), S. Levine et al. <a class="reference external" href="https://arxiv.org/1603.02199">[pdf]</a></p></li>
<li><p>Asynchronous methods for deep reinforcement learning (2016), V. Mnih et al. <a class="reference external" href="http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf">[pdf]</a></p></li>
<li><p>Deep Reinforcement Learning with Double Q-Learning (2016), H. Hasselt et al. <a class="reference external" href="https://arxiv.org/1509.06461.pdf">[pdf]</a></p></li>
<li><p>Mastering the game of Go with deep neural networks and tree search (2016), D. Silver et al. <a class="reference external" href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">[pdf]</a></p></li>
<li><p>Continuous control with deep reinforcement learning (2015), T. Lillicrap et al. <a class="reference external" href="https://arxiv.org/1509.02971">[pdf]</a></p></li>
<li><p>Human-level control through deep reinforcement learning (2015), V. Mnih et al. <a class="reference external" href="http://www.davidqiu.com:8888/research/nature14236.pdf">[pdf]</a></p></li>
<li><p>Deep learning for detecting robotic grasps (2015), I. Lenz et al. <a class="reference external" href="http://www.cs.cornell.edu/~asaxena/papers/lenz_lee_saxena_deep_learning_grasping_ijrr2014.pdf">[pdf]</a></p></li>
<li><p>Playing atari with deep reinforcement learning (2013), V. Mnih et al. <a class="reference external" href="http://arxiv.org/1312.5602.pdf)">[pdf]</a></p></li>
</ul>
</section>
<section id="new-papers">
<h3><a class="toc-backref" href="#id12">New papers</a><a class="headerlink" href="#new-papers" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Deep Photo Style Transfer (2017), F. Luan et al. <a class="reference external" href="http://arxiv.org/1703.07511v1.pdf">[pdf]</a></p></li>
<li><p>Evolution Strategies as a Scalable Alternative to Reinforcement Learning (2017), T. Salimans et al. <a class="reference external" href="http://arxiv.org/1703.03864v1.pdf">[pdf]</a></p></li>
<li><p>Deformable Convolutional Networks (2017), J. Dai et al. <a class="reference external" href="http://arxiv.org/1703.06211v2.pdf">[pdf]</a></p></li>
<li><p>Mask R-CNN (2017), K. He et al. <a class="reference external" href="https://128.84.21.199/1703.06870">[pdf]</a></p></li>
<li><p>Learning to discover cross-domain relations with generative adversarial networks (2017), T. Kim et al. <a class="reference external" href="http://arxiv.org/1703.05192v1.pdf">[pdf]</a></p></li>
<li><p>Deep voice: Real-time neural text-to-speech (2017), S. Arik et al., <a class="reference external" href="http://arxiv.org/1702.07825v2.pdf">[pdf]</a></p></li>
<li><p>PixelNet: Representation of the pixels, by the pixels, and for the pixels (2017), A. Bansal et al. <a class="reference external" href="http://arxiv.org/1702.06506v1.pdf">[pdf]</a></p></li>
<li><p>Batch renormalization: Towards reducing minibatch dependence in batch-normalized models (2017), S. Ioffe. <a class="reference external" href="https://arxiv.org/abs/1702.03275">[pdf]</a></p></li>
<li><p>Wasserstein GAN (2017), M. Arjovsky et al. <a class="reference external" href="https://arxiv.org/1701.07875v1">[pdf]</a></p></li>
<li><p>Understanding deep learning requires rethinking generalization (2017), C. Zhang et al. <a class="reference external" href="https://arxiv.org/1611.03530">[pdf]</a></p></li>
<li><p>Least squares generative adversarial networks (2016), X. Mao et al. <a class="reference external" href="https://arxiv.org/abs/1611.04076v2">[pdf]</a></p></li>
</ul>
</section>
<section id="classic-papers">
<h3><a class="toc-backref" href="#id13">Classic Papers</a><a class="headerlink" href="#classic-papers" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>An analysis of single-layer networks in unsupervised feature learning (2011), A. Coates et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf">[pdf]</a></p></li>
<li><p>Deep sparse rectifier neural networks (2011), X. Glorot et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_GlorotBB11.pdf">[pdf]</a></p></li>
<li><p>Natural language processing (almost) from scratch (2011), R. Collobert et al. <a class="reference external" href="http://arxiv.org/1103.0398">[pdf]</a></p></li>
<li><p>Recurrent neural network based language model (2010), T. Mikolov et al. <a class="reference external" href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">[pdf]</a></p></li>
<li><p>Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion (2010), P. Vincent et al. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.3484&amp;rep=rep1&amp;type=pdf">[pdf]</a></p></li>
<li><p>Learning mid-level features for recognition (2010), Y. Boureau <a class="reference external" href="http://ece.duke.edu/~lcarin/boureau-cvpr-10.pdf">[pdf]</a></p></li>
<li><p>A practical guide to training restricted boltzmann machines (2010), G. Hinton <a class="reference external" href="http://www.csri.utoronto.ca/~hinton/absps/guideTR.pdf">[pdf]</a></p></li>
<li><p>Understanding the difficulty of training deep feedforward neural networks (2010), X. Glorot and Y. Bengio <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf">[pdf]</a></p></li>
<li><p>Why does unsupervised pre-training help deep learning (2010), D. Erhan et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_ErhanCBV10.pdf">[pdf]</a></p></li>
<li><p>Learning deep architectures for AI (2009), Y. Bengio. <a class="reference external" href="http://sanghv.com/download/soft/machine%20learning,%20artificial%20intelligence,%20mathematics%20ebooks/ML/learning%20deep%20architectures%20for%20AI%20(2009).pdf">[pdf]</a></p></li>
<li><p>Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009), H. Lee et al. <a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;rep=rep1&amp;type=pdf">[pdf]</a></p></li>
<li><p>Greedy layer-wise training of deep networks (2007), Y. Bengio et al. <a class="reference external" href="http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2006_739.pdf">[pdf]</a></p></li>
<li><p>A fast learning algorithm for deep belief nets (2006), G. Hinton et al. <a class="reference external" href="http://nuyoo.utm.mx/~jjf/rna/A8%20A%20fast%20learning%20algorithm%20for%20deep%20belief%20nets.pdf">[pdf]</a></p></li>
<li><p>Gradient-based learning applied to document recognition (1998), Y. LeCun et al. <a class="reference external" href="http://yann.lecun.com/exdb/publis/lecun-01a.pdf">[pdf]</a></p></li>
<li><p>Long short-term memory (1997), S. Hochreiter and J. Schmidhuber. <a class="reference external" href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/neco.1997.9.8.1735">[pdf]</a></p></li>
</ul>
</section>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="libraries.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Libraries</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="other_content.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Other Content</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Team<br/>
  
      &copy; Copyright 2017.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>