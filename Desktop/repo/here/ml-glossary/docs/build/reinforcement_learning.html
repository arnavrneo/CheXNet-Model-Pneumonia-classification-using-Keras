
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Reinforcement Learning &#8212; üè† ML Glossary</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/theme_overrides.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Datasets" href="datasets.html" />
    <link rel="prev" title="Regression Algorithms" href="regression_algos.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"><br/>
<a href="https://github.com/bfortuner/ml-glossary">
    
    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 496 512"><!-- Font Awesome Pro 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    Edit on GitHub
</a></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">üè† ML Glossary</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="linear_regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient_descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic_regression.html">
   Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   Glossary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="calculus.html">
   Calculus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability.html">
   Probability (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statistics.html">
   Statistics (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="math_notation.html">
   Notation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nn_concepts.html">
   Concepts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="forwardpropagation.html">
   Forwardpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backpropagation.html">
   Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="activation_functions.html">
   Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loss_functions.html">
   Loss Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optimizers.html">
   Optimizers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regularization.html">
   Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="architectures.html">
   Architectures
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Algorithms (TODO)
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="classification_algos.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="clustering_algos.html">
   Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_algos.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Reinforcement Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="datasets.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="libraries.html">
   Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="papers.html">
   Papers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="other_content.html">
   Other
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="contribute.html">
   How to contribute
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/reinforcement_learning.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#note-on-terminology">
   Note on Terminology
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#eploration-vs-exploitation">
   Eploration vs. Exploitation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mdps-and-tabular-methods">
   MDPs and Tabular methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#monte-carlo-methods">
   Monte Carlo methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#temporal-difference-learning">
   Temporal-Difference Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#planning">
   Planning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#on-policy-vs-off-policy-learning">
   On-Policy vs. Off-Policy Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-free-vs-model-based-approaches">
   Model-Free vs. Model-Based Approaches
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imitation-learning">
   Imitation Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#q-learning">
   Q-Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-q-learning">
   Deep Q-Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples-of-applications">
   Examples of Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#links">
   Links
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Reinforcement Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#note-on-terminology">
   Note on Terminology
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#eploration-vs-exploitation">
   Eploration vs. Exploitation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mdps-and-tabular-methods">
   MDPs and Tabular methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#monte-carlo-methods">
   Monte Carlo methods
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#temporal-difference-learning">
   Temporal-Difference Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#planning">
   Planning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#on-policy-vs-off-policy-learning">
   On-Policy vs. Off-Policy Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-free-vs-model-based-approaches">
   Model-Free vs. Model-Based Approaches
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#imitation-learning">
   Imitation Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#q-learning">
   Q-Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#deep-q-learning">
   Deep Q-Learning
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#examples-of-applications">
   Examples of Applications
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#links">
   Links
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="reinforcement-learning">
<span id="id1"></span><h1>Reinforcement Learning<a class="headerlink" href="#reinforcement-learning" title="Permalink to this headline">#</a></h1>
<p>In machine learning, supervised is sometimes contrasted with unsupervised learning. This is a useful distinction, but there are some problem domains that have share characteristics with each without fitting exactly in either category. In cases where the algorithm does not have explicit labels but does receive a form of feedback, we are dealing with a third and distinct paradigm of machine learning - reinforcement learning.</p>
<p>Programmatic and a theoretical introduction to reinforcement learning:https://spinningup.openai.com/</p>
<p>There are different problem types and algorithms, but all reinforcement learning problems have the following aspects in common:</p>
<blockquote>
<div><ul class="simple">
<li><p>an <strong>agent</strong> - the algorithm or ‚ÄúAI‚Äù responsible for making decisions</p></li>
<li><p>an <strong>environment</strong>, consisting of different <strong>states</strong> in which the agent may find itself</p></li>
<li><p>a <strong>reward</strong> signal which is returned by the environment as a function of the current state</p></li>
<li><p><strong>actions</strong>, each of which takes the agent from one state to another</p></li>
<li><p>a <strong>policy</strong>, i.e. a mapping from states to actions that defines the agent‚Äôs behavior</p></li>
</ul>
</div></blockquote>
<p>The goal of reinforcement learning is to learn the optimal policy, that is the policy that maximizes expected (discounted) cumulative reward.</p>
<p>Many RL algorithms will include a value function or a Q-function. A value function gives the expected cumulative reward for each state under the current policy In other words, it answers the question, ‚ÄúIf I begin in state <span class="math notranslate nohighlight">\(i\)</span> and follow my policy, what will be my expected reward?‚Äù</p>
<p>In most algorithms, expected cumulative reward is discounted by some factor <span class="math notranslate nohighlight">\(\gamma \in (0, 1)\)</span>; a typical value for <span class="math notranslate nohighlight">\(\gamma\)</span> is 0.9. In addition to more accurately modeling the behavior of humans and other animals, <span class="math notranslate nohighlight">\(\gamma &lt; 1\)</span> helps to ensure that algorithms converge even when there is no terminal state or when the terminal state is never found (because otherwise expected cumulative reward may also become infinite).</p>
<section id="note-on-terminology">
<h2>Note on Terminology<a class="headerlink" href="#note-on-terminology" title="Permalink to this headline">#</a></h2>
<p>For mostly historical reasons, engineering and operations research use different words to talk about the same concepts. For example, the general field of reinforcement learning itself is sometimes referred to as optimal control, approximate dynamic programming, or neuro-dynamic programming.<sup>1</sup></p>
</section>
<section id="eploration-vs-exploitation">
<h2>Eploration vs. Exploitation<a class="headerlink" href="#eploration-vs-exploitation" title="Permalink to this headline">#</a></h2>
<p>One dilemma inherent to the RL problem setting is the tension between the desire to choose the best known option and the need to try something new in order to discover other options that may be even better. Choosing the best known action is known as exploitation, while choosing a different action is known as exploration.</p>
<p>Typically, this is solved by adding to the policy a small probability of exploration. For example, the policy could be to choose the optimal action (optimal with regard to what is known) with probability 0.95, and exploring by randomly choosing some other action with probability 0.5 (if uniform across all remaining actions: probability 0.5/(n-1) where n is the number of states).</p>
</section>
<section id="mdps-and-tabular-methods">
<h2>MDPs and Tabular methods<a class="headerlink" href="#mdps-and-tabular-methods" title="Permalink to this headline">#</a></h2>
<p>Many problems can be effectively modeled as Markov Decision Processes (MDPs), and usually as <cite>Partially Observable Markov Decision Processes (POMDPs) &lt;https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process&gt;</cite>. That is, we have</p>
<blockquote>
<div><ul class="simple">
<li><p>a set of states <span class="math notranslate nohighlight">\(S\)</span></p></li>
<li><p>a set of actions <span class="math notranslate nohighlight">\(A\)</span></p></li>
<li><p>a set of conditional state transition probabilities <span class="math notranslate nohighlight">\(T\)</span></p></li>
<li><p>a reward function <span class="math notranslate nohighlight">\(R: S \times A \rightarrow \mathbb{R}\)</span></p></li>
<li><p>a set of observations <span class="math notranslate nohighlight">\(\Omega\)</span></p></li>
<li><p>a set of condition observation probabilities <span class="math notranslate nohighlight">\(O\)</span></p></li>
<li><p>a discount factor <span class="math notranslate nohighlight">\(\gamma \in [0]\)</span></p></li>
</ul>
</div></blockquote>
<p>Given these things, the goal is to choose the action at each time step which will maximize <span class="math notranslate nohighlight">\(E \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]\)</span>, the expected discounted reward.</p>
</section>
<section id="monte-carlo-methods">
<h2>Monte Carlo methods<a class="headerlink" href="#monte-carlo-methods" title="Permalink to this headline">#</a></h2>
<p>One possible approach is to run a large number of simulations to learn <span class="math notranslate nohighlight">\(p^*\)</span>. This is good for cases where we know the environment and can run many simulations reasonably quickly. For example, it is fairly trivial to compute an optimal policy for the card game <cite>21 (blackjack) &lt;https://en.wikipedia.org/wiki/Twenty-One_(card_game)&gt;</cite> by running many simulations, and the same is true for most simple games.</p>
</section>
<section id="temporal-difference-learning">
<h2>Temporal-Difference Learning<a class="headerlink" href="#temporal-difference-learning" title="Permalink to this headline">#</a></h2>
<p>TODO</p>
</section>
<section id="planning">
<h2>Planning<a class="headerlink" href="#planning" title="Permalink to this headline">#</a></h2>
<p>TODO</p>
</section>
<section id="on-policy-vs-off-policy-learning">
<h2>On-Policy vs. Off-Policy Learning<a class="headerlink" href="#on-policy-vs-off-policy-learning" title="Permalink to this headline">#</a></h2>
<p>TODO</p>
</section>
<section id="model-free-vs-model-based-approaches">
<h2>Model-Free vs. Model-Based Approaches<a class="headerlink" href="#model-free-vs-model-based-approaches" title="Permalink to this headline">#</a></h2>
<p>TODO</p>
</section>
<section id="imitation-learning">
<h2>Imitation Learning<a class="headerlink" href="#imitation-learning" title="Permalink to this headline">#</a></h2>
<p>TODO</p>
</section>
<section id="q-learning">
<h2>Q-Learning<a class="headerlink" href="#q-learning" title="Permalink to this headline">#</a></h2>
<p>Q Learning, a model-free RL algorithm, is to update Q values to the optimal by iteration. It is an off-policy method that select the optimal action based on the current estimated Q* and does not follow the current policy.</p>
<p>The algorithm of Q Learning is:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Initialize t = 0.</p></li>
<li><p>Start at initial state s<sub>t</sub> = 0.</p></li>
<li><p>The agent chooses a<sub>t</sub> = …õ-greedy
action.</p></li>
<li><p>For given a<sub>t</sub>, the agent retrieves
the reward r<sub>t+1</sub> as well as the next
state s<sub>t+1</sub>.</p></li>
<li><p>Get (but do not perform) the next action
a<sub>t+1</sub> =
argmax<sub>a‚ààA</sub>Q(s<sub>t+1</sub>, a).</p></li>
<li><p>Compute the TD target y<sub>t</sub> =
r<sub>t+1</sub> + Œ≥ ¬∑ Q(s<sub>t+1</sub>,
a<sub>t+1</sub>), where Œ≥ is the discounted
factor.</p></li>
<li><p>Calculate the TD error Œ¥ = y<sub>t</sub> ‚àí
Q(s<sub>t</sub>, a<sub>t</sub>).</p></li>
<li><p>Update Q(s<sub>t</sub>, a<sub>t</sub>) ‚Üê
Q(s<sub>t</sub>, a<sub>t</sub>) + Œ±<sub>t</sub> ¬∑
Œ¥, where Œ±<sub>t</sub> is the step size
(learning rate) at t.</p></li>
<li><p>Update t ‚Üê t + 1 and repeat step 3-9 until
Q(s, a) converge.</p></li>
</ol>
</div></blockquote>
<p>Epsilon-Greedy Algorithm</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{equation}
a_{t} = \begin{cases}
argmax_{a‚ààA} &amp; \text{if } p = 1 - e \\
random\, action\ &amp;\text{otherwise}
\end{cases}
\end{equation}\end{split}\]</div>
<p>The agent performs optimal action for exploitation or random action for exploration during training. It acts randomly in the beginning with the …õ = 1 and chooses the best action based on the Q function with a decreasing …õ capped at some small constant not equal to zero.</p>
<p>Q-Table / Q-Matrix</p>
<blockquote>
<div><table class="table">
<colgroup>
<col style="width: 21%" />
<col style="width: 24%" />
<col style="width: 24%" />
<col style="width: 8%" />
<col style="width: 24%" />
</colgroup>
<tbody>
<tr class="row-odd"><td></td>
<td><p>a<sub>1</sub></p></td>
<td><p>a<sub>2</sub></p></td>
<td><p>‚Ä¶</p></td>
<td><p>a<sub>n</sub></p></td>
</tr>
<tr class="row-even"><td><p>s<sub>1</sub></p></td>
<td><p>Q
(s<sub>1</sub>,
a<sub>1</sub>)</p></td>
<td><p>Q
(s<sub>1</sub>,
a<sub>2</sub>)</p></td>
<td><p>‚Ä¶</p></td>
<td><p>Q
(s<sub>1</sub>,
a<sub>3</sub>)</p></td>
</tr>
<tr class="row-odd"><td><p>s<sub>2</sub></p></td>
<td><p>Q
(s<sub>2</sub>,
a<sub>1</sub>)</p></td>
<td><p>Q
(s<sub>2</sub>,
a<sub>2</sub>)</p></td>
<td><p>‚Ä¶</p></td>
<td><p>Q
(s<sub>2</sub>,
a<sub>3</sub>)</p></td>
</tr>
<tr class="row-even"><td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
<td><p>‚Ä¶</p></td>
</tr>
<tr class="row-odd"><td><p>s<sub>m</sub></p></td>
<td><p>Q
(s<sub>m</sub>,
a<sub>1</sub>)</p></td>
<td><p>Q
(s<sub>m</sub>,
a<sub>2</sub>)</p></td>
<td><p>‚Ä¶</p></td>
<td><p>Q
(s<sub>m</sub>,
a<sub>3</sub>)</p></td>
</tr>
</tbody>
</table>
</div></blockquote>
<p>It‚Äôs a lookup table storing the action-value function Q(s, a) for state-action pairs where there are M states and n actions. We can initialize the Q(s, a) arbitrarily except s = terminal state. For s = final state, we set it equal to the reward on that state.</p>
<p>Reasons of using Q Learning are:</p>
<blockquote>
<div><ul class="simple">
<li><p>It‚Äôs applicable for the discrete action space of our environment.</p></li>
<li><p>When we don‚Äôt have the true MDP model: transitional probability matrix and rewards (Model-Free Setting).</p></li>
<li><p>It‚Äôs able to learn from incomplete episodes because of TD learning.</p></li>
</ul>
</div></blockquote>
<p>Drawbacks of Q Learning are:</p>
<blockquote>
<div><ul class="simple">
<li><p>When the state space and action space are continuous and extremely large, due to the curse of dimensionality, it‚Äôs nearly impossible to maintain a Q-matrix when the data is large.</p></li>
<li><p>Using a Q-table is unable to infer optimal action for unseen states.</p></li>
</ul>
</div></blockquote>
</section>
<section id="deep-q-learning">
<h2>Deep Q-Learning<a class="headerlink" href="#deep-q-learning" title="Permalink to this headline">#</a></h2>
<p>Deep Q-learning pursues the same general methods as Q-learning. Its innovation is to add a neural network, which makes it possible to learn a very complex Q-function. This makes it very powerful, especially because it makes a large body of well-developed theory and tools for deep learning useful to reinforcement learning problems.</p>
</section>
<section id="examples-of-applications">
<h2>Examples of Applications<a class="headerlink" href="#examples-of-applications" title="Permalink to this headline">#</a></h2>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://blog.paperspace.com/creating-custom-environments-openai-gym/">Getting Started With OpenAI Gym: Creating Custom Gym Environments</a></p></li>
<li><p><a class="reference external" href="https://www.simplilearn.com/tutorials/machine-learning-tutorial/what-is-q-learning">What Is Q-Learning: The Best Guide To Understand Q-Learning (Simplilearn)</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html">REINFORCEMENT LEARNING (DQN) TUTORIAL (PyTorch)</a></p></li>
<li><p><a class="reference external" href="https://github.com/yatshunlee/qwop_RL">QWOP Game AI (DQN/DDQN)</a></p></li>
</ul>
</div></blockquote>
</section>
<section id="links">
<h2>Links<a class="headerlink" href="#links" title="Permalink to this headline">#</a></h2>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12">Practical Applications of Reinforcement Learning (tTowards Data Science)</a></p></li>
<li><p><a class="reference external" href="https://www.geeksforgeeks.org/what-is-reinforcement-learning/">Reinforcement learning (GeeksforGeeks)</a></p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;SmartLabAI/reinforcement-learning-algorithms-an-intuitive-overview-904e2dff5bbc">Reinforcement Learning Algorithms: An Intuitive Overview (SmartLabAI)</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Q-learning">Q-learning(Wikipedia)</a></p></li>
<li><p><a class="reference external" href="https://www.geeksforgeeks.org/epsilon-greedy-algorithm-in-reinforcement-learning/">Epsilon-Greedy Algorithm in Reinforcement Learning (GeeksforGeeks)</a></p></li>
<li><p><a class="reference external" href="https://www.gymlibrary.ml/">OpenAI Gym Documentation</a></p></li>
<li><p><a class="reference external" href="https://stable-baselines3.readthedocs.io/en/master/#">Stable-Baselines3 Documentation</a></p></li>
<li><p><a class="reference external" href="https://www.davidsilver.uk/teaching/">David Silver Teaching Material</a></p></li>
</ul>
</div></blockquote>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Reinforcement_learning#Introduction">https://en.wikipedia.org/wiki/Reinforcement_learning#Introduction</a></p>
</dd>
<dt class="label" id="id3"><span class="brackets">2</span></dt>
<dd><p>Reinforcement Learning: An Introduction (Sutton and Barto, 2018)</p>
</dd>
<dt class="label" id="id4"><span class="brackets">3</span></dt>
<dd><p>Silver, David. ‚ÄúLecture 5: Model-Free Control.‚Äù UCL, Computer Sci. Dep. Reinf Learn. Lect. (2015): 101-140.</p>
</dd>
<dt class="label" id="id5"><span class="brackets">4</span></dt>
<dd><p>En.wikipedia.org. 2022. Q-learning - Wikipedia. [online] Available at: &lt;<a class="reference external" href="https://en.wikipedia.org/wiki/Q-learning">https://en.wikipedia.org/wiki/Q-learning</a>&gt; [Accessed 15 June 2022].</p>
</dd>
</dl>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="regression_algos.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Regression Algorithms</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="datasets.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Datasets</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Team<br/>
  
      &copy; Copyright 2017.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>