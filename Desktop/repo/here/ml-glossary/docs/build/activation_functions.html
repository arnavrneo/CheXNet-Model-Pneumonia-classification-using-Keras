
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Activation Functions &#8212; üè† ML Glossary</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/theme_overrides.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Layers" href="layers.html" />
    <link rel="prev" title="Backpropagation" href="backpropagation.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"><br/>
<a href="https://github.com/bfortuner/ml-glossary">
    
    <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 496 512"><!-- Font Awesome Pro 5.15.4 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license (Commercial License) --><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    Edit on GitHub
</a></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">üè† ML Glossary</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="linear_regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="gradient_descent.html">
   Gradient Descent
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="logistic_regression.html">
   Logistic Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="glossary.html">
   Glossary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="calculus.html">
   Calculus
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="linear_algebra.html">
   Linear Algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probability.html">
   Probability (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="statistics.html">
   Statistics (TODO)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="math_notation.html">
   Notation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="nn_concepts.html">
   Concepts
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="forwardpropagation.html">
   Forwardpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backpropagation.html">
   Backpropagation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Activation Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="layers.html">
   Layers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="loss_functions.html">
   Loss Functions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="optimizers.html">
   Optimizers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regularization.html">
   Regularization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="architectures.html">
   Architectures
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Algorithms (TODO)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="classification_algos.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="clustering_algos.html">
   Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression_algos.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="reinforcement_learning.html">
   Reinforcement Learning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="datasets.html">
   Datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="libraries.html">
   Libraries
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="papers.html">
   Papers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="other_content.html">
   Other
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Contributing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="contribute.html">
   How to contribute
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/activation_functions.rst.txt"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.rst</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear">
   Linear
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elu">
   ELU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relu">
   ReLU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#leakyrelu">
   LeakyReLU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sigmoid">
   Sigmoid
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tanh">
   Tanh
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#softmax">
   Softmax
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Activation Functions</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear">
   Linear
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#elu">
   ELU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#relu">
   ReLU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#leakyrelu">
   LeakyReLU
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sigmoid">
   Sigmoid
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tanh">
   Tanh
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#softmax">
   Softmax
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="activation-functions">
<span id="id1"></span><h1>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h1>
<div class="contents local topic" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#linear" id="id6">Linear</a></p></li>
<li><p><a class="reference internal" href="#elu" id="id7">ELU</a></p></li>
<li><p><a class="reference internal" href="#relu" id="id8">ReLU</a></p></li>
<li><p><a class="reference internal" href="#leakyrelu" id="id9">LeakyReLU</a></p></li>
<li><p><a class="reference internal" href="#sigmoid" id="id10">Sigmoid</a></p></li>
<li><p><a class="reference internal" href="#tanh" id="id11">Tanh</a></p></li>
<li><p><a class="reference internal" href="#softmax" id="id12">Softmax</a></p></li>
</ul>
</div>
<section id="linear">
<span id="activation-linear"></span><h2><a class="toc-backref" href="#id6">Linear</a><a class="headerlink" href="#linear" title="Permalink to this headline">#</a></h2>
<p>A straight line function where activation is proportional to input ( which is the weighted sum from neuron ).</p>
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Function</p></td>
<td><p>Derivative</p></td>
</tr>
<tr class="row-even"><td><div class="math notranslate nohighlight">
\[\begin{split}R(z,m) = \begin{Bmatrix} z*m    \\
           \end{Bmatrix}\end{split}\]</div>
</td>
<td><div class="math notranslate nohighlight">
\[\begin{split}R'(z,m) = \begin{Bmatrix} m     \\
            \end{Bmatrix}\end{split}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal image-reference" href="_images/linear.png"><img alt="_images/linear.png" class="align-center" src="_images/linear.png" style="width: 256px; height: 256px;" /></a>
</td>
<td><a class="reference internal image-reference" href="_images/linear_prime.png"><img alt="_images/linear_prime.png" class="align-center" src="_images/linear_prime.png" style="width: 256px; height: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">m</span><span class="o">*</span><span class="n">z</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_prime</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">m</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">m</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li><p>It gives a range of activations, so it is not binary activation.</p></li>
<li><p>We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that.</p></li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li><p>For this function, derivative is a constant. That means, the gradient has no relationship with X.</p></li>
<li><p>It is a constant gradient and the descent is going to be on constant gradient.</p></li>
<li><p>If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x) !</p></li>
</ul>
</section>
<section id="elu">
<span id="activation-elu"></span><h2><a class="toc-backref" href="#id7">ELU</a><a class="headerlink" href="#elu" title="Permalink to this headline">#</a></h2>
<p>Exponential Linear Unit or its widely known name ELU is a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number.</p>
<p>ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -Œ± whereas RELU sharply smoothes.</p>
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Function</p></td>
<td><p>Derivative</p></td>
</tr>
<tr class="row-even"><td><div class="math notranslate nohighlight">
\[\begin{split}R(z) = \begin{Bmatrix} z &amp; z &gt; 0 \\
 Œ±.( e^z ‚Äì 1) &amp; z &lt;= 0 \end{Bmatrix}\end{split}\]</div>
</td>
<td><div class="math notranslate nohighlight">
\[\begin{split}R'(z) = \begin{Bmatrix} 1 &amp; z&gt;0 \\
Œ±.e^z &amp; z&lt;0 \end{Bmatrix}\end{split}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal image-reference" href="_images/elu.png"><img alt="_images/elu.png" class="align-center" src="_images/elu.png" style="width: 256px; height: 256px;" /></a>
</td>
<td><a class="reference internal image-reference" href="_images/elu_prime.png"><img alt="_images/elu_prime.png" class="align-center" src="_images/elu_prime.png" style="width: 256px; height: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">z</span> <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">alpha</span><span class="o">*</span><span class="p">(</span><span class="n">e</span><span class="o">^</span><span class="n">z</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">elu_prime</span><span class="p">(</span><span class="n">z</span><span class="p">,</span><span class="n">alpha</span><span class="p">):</span>
	<span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">alpha</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li><p>ELU becomes smooth slowly until its output equal to -Œ± whereas RELU sharply smoothes.</p></li>
<li><p>ELU is a strong alternative to ReLU.</p></li>
<li><p>Unlike to ReLU, ELU can produce negative outputs.</p></li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li><p>For x &gt; 0, it can blow up the activation with the output range of [0, inf].</p></li>
</ul>
</section>
<section id="relu">
<span id="activation-relu"></span><h2><a class="toc-backref" href="#id8">ReLU</a><a class="headerlink" href="#relu" title="Permalink to this headline">#</a></h2>
<p>A recent invention which stands for Rectified Linear Units. The formula is deceptively simple: <span class="math notranslate nohighlight">\(max(0,z)\)</span>. Despite its name and appearance, it‚Äôs not linear and provides the same benefits as Sigmoid (i.e. the ability to learn nonlinear functions), but with better performance.</p>
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Function</p></td>
<td><p>Derivative</p></td>
</tr>
<tr class="row-even"><td><div class="math notranslate nohighlight">
\[\begin{split}R(z) = \begin{Bmatrix} z &amp; z &gt; 0 \\
 0 &amp; z &lt;= 0 \end{Bmatrix}\end{split}\]</div>
</td>
<td><div class="math notranslate nohighlight">
\[\begin{split}R'(z) = \begin{Bmatrix} 1 &amp; z&gt;0 \\
0 &amp; z&lt;0 \end{Bmatrix}\end{split}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal image-reference" href="_images/relu.png"><img alt="_images/relu.png" class="align-center" src="_images/relu.png" style="width: 256px; height: 256px;" /></a>
</td>
<td><a class="reference internal image-reference" href="_images/relu_prime.png"><img alt="_images/relu_prime.png" class="align-center" src="_images/relu_prime.png" style="width: 256px; height: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">relu_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li><p>It avoids and rectifies vanishing gradient problem.</p></li>
<li><p>ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations.</p></li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li><p>One of its limitations is that it should only be used within hidden layers of a neural network model.</p></li>
<li><p>Some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. In other words, ReLu can result in dead neurons.</p></li>
<li><p>In another words, For activations in the region (x&lt;0) of ReLu, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/ input (simply because gradient is 0, nothing changes). This is called the dying ReLu problem.</p></li>
<li><p>The range of ReLu is <span class="math notranslate nohighlight">\([0, \infty)\)</span>. This means it can blow up the activation.</p></li>
</ul>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><p><a class="reference external" href="http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf">Deep Sparse Rectifier Neural Networks</a> Glorot et al., (2011)</p></li>
<li><p><a class="reference external" href="https://medium.com/&#64;karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes You Should Understand Backprop</a>, Karpathy (2016)</p></li>
</ul>
</section>
<section id="leakyrelu">
<span id="activation-leakyrelu"></span><h2><a class="toc-backref" href="#id9">LeakyReLU</a><a class="headerlink" href="#leakyrelu" title="Permalink to this headline">#</a></h2>
<p>LeakyRelu is a variant of ReLU. Instead of being 0 when <span class="math notranslate nohighlight">\(z &lt; 0\)</span>, a leaky ReLU allows a small, non-zero, constant gradient <span class="math notranslate nohighlight">\(\alpha\)</span> (Normally, <span class="math notranslate nohighlight">\(\alpha = 0.01\)</span>). However, the consistency of the benefit across tasks is presently unclear. <a class="footnote-reference brackets" href="#id5" id="id2">1</a></p>
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Function</p></td>
<td><p>Derivative</p></td>
</tr>
<tr class="row-even"><td><div class="math notranslate nohighlight">
\[\begin{split}R(z) = \begin{Bmatrix} z &amp; z &gt; 0 \\
 \alpha z &amp; z &lt;= 0 \end{Bmatrix}\end{split}\]</div>
</td>
<td><div class="math notranslate nohighlight">
\[\begin{split}R'(z) = \begin{Bmatrix} 1 &amp; z&gt;0 \\
\alpha &amp; z&lt;0 \end{Bmatrix}\end{split}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal image-reference" href="_images/leakyrelu.png"><img alt="_images/leakyrelu.png" class="align-center" src="_images/leakyrelu.png" style="width: 256px; height: 256px;" /></a>
</td>
<td><a class="reference internal image-reference" href="_images/leakyrelu_prime.png"><img alt="_images/leakyrelu_prime.png" class="align-center" src="_images/leakyrelu_prime.png" style="width: 256px; height: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">leakyrelu</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
	<span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="n">alpha</span> <span class="o">*</span> <span class="n">z</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">leakyrelu_prime</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="p">):</span>
	<span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">z</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">alpha</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li><p>Leaky ReLUs are one attempt to fix the ‚Äúdying ReLU‚Äù problem by having a small negative slope (of 0.01, or so).</p></li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li><p>As it possess linearity, it can‚Äôt be used for the complex Classification. It lags behind the Sigmoid and Tanh for some of the use cases.</p></li>
</ul>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1502.01852.pdf">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>, Kaiming He et al. (2015)</p></li>
</ul>
</section>
<section id="sigmoid">
<span id="activation-sigmoid"></span><h2><a class="toc-backref" href="#id10">Sigmoid</a><a class="headerlink" href="#sigmoid" title="Permalink to this headline">#</a></h2>
<p>Sigmoid takes a real value as input and outputs another value between 0 and 1. It‚Äôs easy to work with and has all the nice properties of activation functions: it‚Äôs non-linear, continuously differentiable, monotonic, and has a fixed output range.</p>
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Function</p></td>
<td><p>Derivative</p></td>
</tr>
<tr class="row-even"><td><div class="math notranslate nohighlight">
\[S(z) = \frac{1} {1 + e^{-z}}\]</div>
</td>
<td><div class="math notranslate nohighlight">
\[S'(z) = S(z) \cdot (1 - S(z))\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal image-reference" href="_images/sigmoid.png"><img alt="_images/sigmoid.png" class="align-center" src="_images/sigmoid.png" style="width: 256px;" /></a>
</td>
<td><a class="reference internal image-reference" href="_images/sigmoid_prime.png"><img alt="_images/sigmoid_prime.png" class="align-center" src="_images/sigmoid_prime.png" style="width: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li><p>It is nonlinear in nature. Combinations of this function are also nonlinear!</p></li>
<li><p>It will give an analog activation unlike step function.</p></li>
<li><p>It has a smooth gradient too.</p></li>
<li><p>It‚Äôs good for a classifier.</p></li>
<li><p>The output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. So we have our activations bound in a range. Nice, it won‚Äôt blow up the activations then.</p></li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li><p>Towards either end of the sigmoid function, the Y values tend to respond very less to changes in X.</p></li>
<li><p>It gives rise to a problem of ‚Äúvanishing gradients‚Äù.</p></li>
<li><p>Its output isn‚Äôt zero centered. It makes the gradient updates go too far in different directions. 0 &lt; output &lt; 1, and it makes optimization harder.</p></li>
<li><p>Sigmoids saturate and kill gradients.</p></li>
<li><p>The network refuses to learn further or is drastically slow ( depending on use case and until gradient /computation gets hit by floating point value limits ).</p></li>
</ul>
<p class="rubric">Further reading</p>
<ul class="simple">
<li><p><a class="reference external" href="https://medium.com/&#64;karpathy/yes-you-should-understand-backprop-e2f06eab496b">Yes You Should Understand Backprop</a>, Karpathy (2016)</p></li>
</ul>
</section>
<section id="tanh">
<span id="activation-tanh"></span><h2><a class="toc-backref" href="#id11">Tanh</a><a class="headerlink" href="#tanh" title="Permalink to this headline">#</a></h2>
<p>Tanh squashes a real-valued number to the range [-1, 1]. It‚Äôs non-linear. But unlike Sigmoid, its output is zero-centered.
Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. <a class="footnote-reference brackets" href="#id5" id="id4">1</a></p>
<table class="table">
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p>Function</p></td>
<td><p>Derivative</p></td>
</tr>
<tr class="row-even"><td><div class="math notranslate nohighlight">
\[tanh(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\]</div>
</td>
<td><div class="math notranslate nohighlight">
\[tanh'(z) = 1 - tanh(z)^{2}\]</div>
</td>
</tr>
<tr class="row-odd"><td><a class="reference internal image-reference" href="_images/tanh.png"><img alt="_images/tanh.png" class="align-center" src="_images/tanh.png" style="width: 256px;" /></a>
</td>
<td><a class="reference internal image-reference" href="_images/tanh_prime.png"><img alt="_images/tanh_prime.png" class="align-center" src="_images/tanh_prime.png" style="width: 256px;" /></a>
</td>
</tr>
<tr class="row-even"><td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
	<span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">tanh_prime</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
	<span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Pros</p>
<ul class="simple">
<li><p>The gradient is stronger for tanh than sigmoid ( derivatives are steeper).</p></li>
</ul>
<p class="rubric">Cons</p>
<ul class="simple">
<li><p>Tanh also has the vanishing gradient problem.</p></li>
</ul>
</section>
<section id="softmax">
<h2><a class="toc-backref" href="#id12">Softmax</a><a class="headerlink" href="#softmax" title="Permalink to this headline">#</a></h2>
<p>Softmax function calculates the probabilities distribution of the event over ‚Äòn‚Äô different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id5"><span class="brackets">1</span><span class="fn-backref">(<a href="#id2">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p><a class="reference external" href="http://cs231n.github.io/neural-networks-1/">http://cs231n.github.io/neural-networks-1/</a></p>
</dd>
</dl>
</section>
</section>


              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="backpropagation.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Backpropagation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="layers.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Layers</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Team<br/>
  
      &copy; Copyright 2017.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>